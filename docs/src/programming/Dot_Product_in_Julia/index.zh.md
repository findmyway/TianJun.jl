# å¦‚ä½•åœ¨Juliaä¸­è®¡ç®—ç‚¹ç§¯?

*å›å­—æœ‰å‡ ç§å†™æ³•?* ğŸ¤”

```@blog_meta
last_update="2021-11-16"
create="2021-11-16"
tags=["Julia", "CUDA", "GPU"]
```

ä¸¤ä¸ªå‘é‡$\vec{a} = [a_1, a_2, ..., a_n]$ å’Œ $\vec{b} = [b_1, b_2, ..., b_n]$ ä¹‹é—´ç‚¹ç§¯ï¼ˆdot product)çš„ä»£æ•°å®šä¹‰å¦‚ä¸‹ï¼š

```math
\vec{a} \cdot \vec{b} = \sum^n_{i=1} a_i b_i = a_1 b_1 + a_2 b_2 + ... + a_n b_n
```

é‚£ä¹ˆï¼Œå¦‚ä½•åœ¨Juliaä¸­å¿«é€Ÿè®¡ç®—ç‚¹ç§¯å‘¢ï¼Ÿ

## ç‰ˆæœ¬1ï¼š ä½¿ç”¨ LinearAlgebra æ ‡å‡†åº“ä¸­çš„ `dot` å‡½æ•°

```julia
julia> using LinearAlgebra

julia> N = 1024*1024
1048576

julia> x, y = rand(N), rand(N);

julia> dot(x,y)
262311.47579656926

```

å…ˆæµ‹è¯•ä¸‹æ ‡å‡†åº“é‡Œ `dot` çš„æ€§èƒ½ï¼š

```julia
julia> using BenchmarkTools

julia> @benchmark dot($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  244.474 Î¼s â€¦  43.973 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     252.275 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   314.178 Î¼s Â± 884.297 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–‡â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â– â–â–‚                                               â–‚
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–†â–‡â–‡â–…â–ƒâ–…â–…â–„â–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–…â–‡â–ˆâ–ˆâ–†â–…â–…â–ƒâ–„â–â–ƒâ–ƒâ–ƒ â–ˆ
  244 Î¼s        Histogram: log(frequency) by time        594 Î¼s <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

ä¸­é—´å€¼ä½äº252Î¼sé™„è¿‘ã€‚

## ç‰ˆæœ¬2ï¼š forå¾ªç¯

å½“ç„¶ï¼Œå³ä½¿ä¸ä½¿ç”¨è‡ªå¸¦çš„`dot`å‡½æ•°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¾ˆæ–¹ä¾¿åœ°ç”¨ä¸€ä¸ª `for` å¾ªç¯æ¥å®ç°ï¼š

```julia
function dot2_1(x, y)
    res = 0
    for i in eachindex(x, y)
        res += x[i] * y[i]
    end
    res
end
```

å†™æ³•åŸºæœ¬å’ŒåŸå§‹çš„æ•°å­¦è¡¨è¾¾å¼ä¸€æ ·ï¼Œé‚£æ€§èƒ½å¦‚ä½•å‘¢ï¼Ÿ

```julia
julia> @benchmark dot2_1($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 2134 samples with 1 evaluation.
 Range (min â€¦ max):  2.286 ms â€¦  2.942 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     2.302 ms              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   2.330 ms Â± 56.418 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–ˆ â–ˆ                                                         
  â–ˆâ–ƒâ–ˆâ–‡â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚ â–ƒ
  2.29 ms        Histogram: frequency by time         2.6 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

å‘ƒï¼Œ è€—æ—¶å·®ä¸å¤šæ˜¯åŸæ¥çš„9å€äº†ã€‚æœ‰ç‚¹ä¸å¯æ€è®®ï¼Œé‚£æ€ä¹ˆä¼˜åŒ–ä¸‹å‘¢ï¼Ÿ å…ˆç”¨ `@code_warntype` çœ‹ä¸‹ï¼š

![](./code_warntype_dot2_1.png)

æ³¨æ„åˆ°ä¸Šé¢æ ‡çº¢è‰²çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯æé†’æˆ‘ä»¬ä¸Šé¢çš„å®ç°ä¸­å‡ºç°äº†ç±»å‹ä¸ç¨³å®šçš„æƒ…å†µã€‚ä¸»è¦åŸå› æ˜¯`res`åœ¨`dot2_1`å‡½æ•°ä¸­ï¼Œåˆå§‹åŒ–æˆäº†`Int64`ç±»å‹çš„`0`ï¼Œè€Œæˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸¤ä¸ª`Vector{Float64}`ç±»å‹çš„å‘é‡ã€‚äº†è§£è¿™ä¸€ç‚¹ä¹‹åï¼Œå¯ä»¥æŠŠä¸Šé¢çš„å®ç°å†™å¾—æ›´çµæ´»ä¸€äº›ï¼š

```julia
function dot2_2(x::AbstractArray{X}, y::AbstractArray{Y}) where {X,Y}
    res = zero(promote_type(X,Y))
    for i in eachindex(x, y)
        res += x[i] * y[i]
    end
    res
end
```

è¿™é‡Œï¼Œ é€šè¿‡ `promote_type` è·å–ç±»å‹ä¿¡æ¯ã€‚

```julia
julia> @benchmark dot2_2($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 3384 samples with 1 evaluation.
 Range (min â€¦ max):  1.410 ms â€¦  3.580 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     1.449 ms              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   1.464 ms Â± 66.969 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

         â–† â–ƒâ–ˆ â–„                                               
  â–‚â–ƒâ–ƒâ–ƒâ–…â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–ˆâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚ â–ƒ
  1.41 ms        Histogram: frequency by time        1.59 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

å¯ä»¥çœ‹åˆ°ï¼Œæ¯”ä¹‹å‰ç¨å¥½äº†ä¸€äº›ï¼Œå¤§çº¦æ˜¯ä¹‹å‰çš„5å€å·¦å³ã€‚ å½“ç„¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é¡ºæ‰‹åšäº›è¿›ä¸€æ­¥çš„ä¼˜åŒ–ï¼ŒåŠ ä¸Š`@simd`å¹¶å»æ‰è¾¹ç•Œæ£€æŸ¥ï¼š

```julia
function dot2_3(x::AbstractArray{X}, y::AbstractArray{Y}) where {X,Y}
    res = zero(promote_type(X,Y))
    @inbounds @simd for i in eachindex(x, y)
        res += x[i] * y[i]
    end
    res
end
```

```julia
julia> @benchmark dot2_3($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 5545 samples with 1 evaluation.
 Range (min â€¦ max):  848.684 Î¼s â€¦  1.296 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     871.641 Î¼s              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   888.964 Î¼s Â± 50.935 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–‚â–‡â–ˆâ–‡â–†â–‡â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–                                       â–‚
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–…â–‡â–‡â–…â–…â–…â–„â–„â–…â–…â–…â–…â–…â–„â–ƒ â–ˆ
  849 Î¼s        Histogram: log(frequency) by time      1.11 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

è¿™æ ·å·®è·è¿›ä¸€æ­¥ç¼©å°äº†ä¸€äº›ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨ [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl) è¿™ä¸ªåº“æ¥æé€Ÿï¼š

```julia
using LoopVectorization
function dot2_4(x::AbstractArray{X}, y::AbstractArray{Y}) where {X,Y}
    res = zero(promote_type(X,Y))
    @turbo for i in eachindex(x, y)
        res += x[i] * y[i]
    end
    res
end
```

```julia
julia> @benchmark dot2_4($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 5905 samples with 1 evaluation.
 Range (min â€¦ max):  802.618 Î¼s â€¦  1.211 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     820.607 Î¼s              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   832.880 Î¼s Â± 39.868 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

   â–‚â–ˆâ–„                                                          
  â–‚â–ˆâ–ˆâ–ˆâ–†â–†â–‡â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â– â–‚
  803 Î¼s          Histogram: frequency by time         1.02 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

çœ‹èµ·æ¥ç¨å¾®å¿«äº†ä¸€äº›ï¼Œä¸è¿‡ä¼¼ä¹ä»ç„¶ä¸`LinearAlgebra`ä¸­çš„æ€§èƒ½æœ‰3å€å¤šçš„æ€§èƒ½å·®è·ï¼Ÿå…¶å®ä¸ç„¶ï¼Œ`LinearAlgebra`ä¸­ä½¿ç”¨äº†BLASï¼Œè€Œå…¶é»˜è®¤æ˜¯æœ‰å¤šçº¿ç¨‹åŠ é€Ÿçš„ï¼Œä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œå¯ä»¥å°†å…¶çº¿ç¨‹æ•°è®¾ç½®ä¸º1ï¼Œç„¶åå¯¹æ¯”ï¼š

```julia
julia> LinearAlgebra.BLAS.set_num_threads(1)

julia> @benchmark dot($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 5980 samples with 1 evaluation.
 Range (min â€¦ max):  795.374 Î¼s â€¦  1.104 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     811.659 Î¼s              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   823.303 Î¼s Â± 37.397 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

   â–…â–ˆ                                                           
  â–ƒâ–ˆâ–ˆâ–ˆâ–…â–‡â–‡â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚ â–ƒ
  795 Î¼s          Histogram: frequency by time         1.02 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

å¯ä»¥çœ‹åˆ°ï¼ŒäºŒè€…ç›¸å·®æ— å‡ ã€‚

## ç‰ˆæœ¬3ï¼š ä¸€è¡Œä»£ç 

å½“ç„¶ï¼Œæœ‰çš„æ—¶å€™å…¶å®å¯¹æ€§èƒ½ä¹Ÿä¸æ˜¯é‚£ä¹ˆå…³å¿ƒï¼Œåè€Œä»£ç çš„ç®€æ´æ€§æ›´é‡è¦ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥ç®€å•åœ°ç”¨ä¸€è¡Œä»£ç æ¥æå®šï¼š

```julia
julia> @benchmark sum(a*b for (a,b) in zip($(rand(N)),$(rand(N))))
BenchmarkTools.Trial: 3823 samples with 1 evaluation.
 Range (min â€¦ max):  1.232 ms â€¦  1.840 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     1.281 ms              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   1.295 ms Â± 54.505 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

   â–ƒâ–ˆâ–‡â–„â–†â–‚                                                     
  â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–„â–„â–„â–…â–…â–„â–…â–„â–…â–†â–†â–†â–…â–…â–†â–‡â–†â–†â–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â– â–ƒ
  1.23 ms        Histogram: frequency by time        1.46 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

## ç‰ˆæœ¬4ï¼š å¤šçº¿ç¨‹

å—å‰é¢LinearAlgebraä¸­å¤šçº¿ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬åŒæ ·ä¹Ÿå¯ä»¥ç”¨Juliaè‡ªå¸¦çš„å¤šçº¿ç¨‹å®Œæˆç‚¹ç§¯çš„è®¡ç®—ã€‚ä¸è¿‡éœ€è¦è®°å¾—åœ¨å¯åŠ¨Juliaçš„æ—¶å€™ï¼Œé€šè¿‡ `-t auto` æ¥æŒ‡å®šçº¿ç¨‹æ•°ã€‚

```julia
julia> using Base.Threads

julia> nthreads()
4
```

è¿™é‡Œæˆ‘æœ¬æœºå°±4ä¸ªçº¿ç¨‹ã€‚æ‰€ä»¥

```julia
function dot4_1(x::AbstractArray{X}, y::AbstractArray{Y}) where {X,Y}
    res = zero(promote_type(X,Y))
    @threads for i in eachindex(x, y)
        res += x[i] * y[i]
    end
    res
end
```

```julia
julia> @benchmark dot4_1($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 109 samples with 1 evaluation.
 Range (min â€¦ max):  28.586 ms â€¦ 113.851 ms  â”Š GC (min â€¦ max):  0.00% â€¦ 62.77%
 Time  (median):     39.838 ms               â”Š GC (median):     0.00%
 Time  (mean Â± Ïƒ):   45.888 ms Â±  20.565 ms  â”Š GC (mean Â± Ïƒ):  14.59% Â± 19.04%

   â–      â–ˆâ–ƒ                                                    
  â–‡â–ˆâ–„â–ˆâ–†â–â–â–‡â–ˆâ–ˆâ–†â–‡â–„â–„â–„â–â–â–â–â–â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–â–â–â–„â–â–ˆâ–‡â–„ â–„
  28.6 ms       Histogram: log(frequency) by time       108 ms <

 Memory estimate: 32.00 MiB, allocs estimate: 2097174.
```

è¿™ä¸ªç»“æœå°±æ¯”è¾ƒæœ‰æ„æ€äº†ï¼Œç”±äºæˆ‘ä»¬çš„å¤šçº¿ç¨‹å®ç°å­˜åœ¨race condition, å®é™…ä¸Šå¾—åˆ°çš„ç»“æœå¹¶ä¸å¯¹ï¼Œå¹¶ä¸”é€Ÿåº¦ç›¸å½“æ…¢ã€‚å½“ç„¶ï¼Œä¸ºäº†ä¿è¯ç»“æœçš„æ­£ç¡®æ€§ï¼Œå¯ä»¥å¯¹`res`åŠ é”ï¼Œä½†å¹¶ä¸èƒ½å¸¦æ¥æ€§èƒ½ä¸Šçš„æå‡ã€‚ä¸€ä¸ªç®€å•çš„åŠæ³•æ˜¯ï¼Œå°†æ•°æ®åˆ†ç‰‡ï¼Œæ¯ä¸ªçº¿ç¨‹åšè‡ªå·±å•ç‹¬çš„è®¡ç®—ï¼Œæœ€åæŠŠå¤šä¸ªçº¿ç¨‹çš„ç»“æœåˆå¹¶ï¼š

```julia
function dot4_2(x::AbstractArray{X}, y::AbstractArray{Y}) where {X,Y}
    res = zeros(promote_type(X,Y), nthreads())
    @threads for i in 1:length(x)
        @inbounds res[threadid()] += x[i] * y[i]
    end
    sum(res)
end
```

```julia
julia> @benchmark dot4_2($(rand(N)), $(rand(N)))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  378.194 Î¼s â€¦  16.460 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     397.990 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   433.403 Î¼s Â± 243.825 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–…â–ˆâ–†â–†â–†â–„â–‚â– â–â–      â–â–‚â–‚â– â–‚â–ƒâ–ƒâ–‚â–                                   â–‚
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–†â–…â–…â–‡â–…â–â–ƒâ–â–ƒâ–ƒâ–â–„â–â–ƒâ–â–â–…â–‡â–„â–…â–ƒâ–ƒâ–„â–…â–„â–â–„â–„â–†â–…â–„â–… â–ˆ
  378 Î¼s        Histogram: log(frequency) by time        878 Î¼s <

 Memory estimate: 1.98 KiB, allocs estimate: 22.
```

è¿™æ ·å¾—åˆ°çš„ç»“æœï¼Œç›¸æ¯”å•çº¿ç¨‹çš„ç»“æœè¦å¿«äº†è¿‘3.2å€ã€‚

Juliaæ ‡å‡†åº“é‡Œæ²¡æœ‰æä¾›å¤šçº¿ç¨‹çš„æ±‚å’Œæ“ä½œï¼Œä¸è¿‡æœ‰ä¸€äº›ç¬¬ä¸‰æ–¹åº“æä¾›äº†è¿™ç±»åŸºæœ¬æ“ä½œï¼Œæ¯”å¦‚[`ThreadsX.jl`](https://github.com/tkf/ThreadsX.jl)ã€‚

```julia
julia> using ThreadsX

julia> @benchmark ThreadsX.sum(a*b for (a,b) in zip($(rand(N)),$(rand(N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  291.519 Î¼s â€¦  2.023 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     326.248 Î¼s              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   339.611 Î¼s Â± 42.828 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

        â–ƒâ–†â–ˆâ–†â–„â–‚â–                                                 
  â–â–‚â–ƒâ–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â– â–ƒ
  292 Î¼s          Histogram: frequency by time          472 Î¼s <

 Memory estimate: 17.45 KiB, allocs estimate: 249.
```

## ç‰ˆæœ¬5ï¼š GPUç‰ˆ

å¦‚æœä½ æ‰‹ä¸Šæ­£å¥½æœ‰å—GPUï¼Œä¸å¦¨è¯•è¯•çœ‹åœ¨GPUä¸Šåšç‚¹ç§¯ã€‚Juliaä¸­çš„[CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)æå¤§åœ°æ–¹ä¾¿äº†Juliaè¯­è¨€é‡Œçš„GPUç¼–ç¨‹ï¼Œé’ˆå¯¹ç‚¹ç§¯è¿™æ ·çš„å¸¸è§æ“ä½œï¼Œå…¶æä¾›äº†åŸºäºcuBLASçš„å°è£…ï¼Œä¸‹é¢æ¥è¯•ä¸‹ï¼š

```julia
julia> using CUDA

julia> @benchmark CUDA.@sync dot($(cu(rand(N))), $(cu(rand(N)))) 
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  26.521 Î¼s â€¦ 68.923 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     27.798 Î¼s              â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   28.331 Î¼s Â±  1.494 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–‚â–   â–ƒâ–‡â–ˆâ–…â– â–ƒâ–…â–‡â–…â–‚   â–‚â–ƒ                                       â–‚
  â–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–„â–„â–…â–†â–†â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–…â–†â–„â–…â–†â–…â–„â–…â–…â–‚â–…â–„â–„â–…â–…â–„â–„â–ƒâ–…â–„ â–ˆ
  26.5 Î¼s      Histogram: log(frequency) by time      35.9 Î¼s <

 Memory estimate: 16 bytes, allocs estimate: 1.
```

å¯ä»¥çœ‹åˆ°ï¼Œå…¶é€Ÿåº¦ç›¸å½“å¿«ã€‚

ä¸è¿‡ï¼Œç”±äºcuBLASé‡Œçš„`dot`åªé’ˆå¯¹å¸¸è§çš„ `Float32`, `Float64`, `Float16` ä»¥åŠå¯¹åº”çš„å¤æ•°ç±»å‹çš„GPUä¸Šçš„å‘é‡æœ‰å®ç°ï¼Œå½“è¾“å…¥çš„ä¸¤ä¸ªå‘é‡çš„å…ƒç´ ç±»å‹ä¸ä¸€è‡´æ—¶ï¼Œç›®å‰çš„CUDA.jl(v3.5.0)ä¼šfallbackåˆ°CPUç‰ˆæœ¬çš„å®ç°ï¼Œå¯¼è‡´æ€§èƒ½ææ…¢ï¼š

```julia
julia> z = rand(Bool, N);

julia> cx, cz = cu(x), cu(z);

julia> @time dot(cx, cz)
â”Œ Warning: Performing scalar indexing on task Task (runnable) @0x00007f63cc0c0010.
â”‚ Invocation of getindex resulted in scalar indexing of a GPU array.
â”‚ This is typically caused by calling an iterating implementation of a method.
â”‚ Such implementations *do not* execute on the GPU, but very slowly on the CPU,
â”‚ and therefore are only permitted from the REPL for prototyping purposes.
â”‚ If you did intend to index this array, annotate the caller with @allowscalar.
â”” @ GPUArrays ~/.julia/packages/GPUArrays/3sW6s/src/host/indexing.jl:56
 12.914170 seconds (6.29 M allocations: 1.000 GiB, 0.87% gc time)
```

ä¸€ä¸ªç®€å•çš„workaroundæ˜¯ï¼Œå…ˆæŠŠç±»å‹ä¸åŒçš„ä¸¤ä¸ªå‘é‡è½¬æ¢æˆç›¸åŒçš„ç±»å‹ï¼Œç„¶åå†è°ƒç”¨`dot`å‡½æ•°ï¼š

```julia
julia> @benchmark CUDA.@sync dot($(cu(rand(N))), convert(CuArray{Float32}, $(cu(rand(Bool, N)))))
BenchmarkTools.Trial: 3968 samples with 1 evaluation.
 Range (min â€¦ max):  1.073 ms â€¦   4.408 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 30.06%
 Time  (median):     1.140 ms               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   1.252 ms Â± 464.709 Î¼s  â”Š GC (mean Â± Ïƒ):  3.35% Â±  6.05%

  â–‚â–ˆâ–ƒ  â–„â–                                     â–                
  â–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–†â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–ˆâ–…â–ƒâ–â–ƒâ–â–â–â–ƒâ–â–ƒâ–â–â–â–ˆ â–ˆ
  1.07 ms      Histogram: log(frequency) by time      3.85 ms <

 Memory estimate: 5.00 MiB, allocs estimate: 12.
```

ç›¸æ¯”åŸæ¥çš„GPUç‰ˆæœ¬ï¼Œå¤šå‡ºæ¥äº†ä¸€æ¬¡æ‹·è´æ•°æ®çš„æ—¶é—´ï¼Œè¿™æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚
ä¸è¿‡ï¼Œ`CUDA.jl`çš„å¼ºå¤§ä¹‹å¤„åœ¨äºï¼Œé’ˆå¯¹è¿™ç±»æ²¡æœ‰å†…ç½®çš„å®ç°ï¼Œæˆ‘ä»¬å¯ä»¥*å¾ˆå®¹æ˜“åœ°*é€šè¿‡ç¼–å†™è‡ªå®šä¹‰çš„æ ¸å‡½æ•°æ¥å®ç°ã€‚

```julia
function dot5_1(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res)
        for i in 1:length(x)
            @inbounds res[] += x[i] * y[i]
        end
    end
    @cuda kernel(x, y, res)
    res[]
end
```

è‡ªå·±æ‰‹å†™æ ¸å‡½æ•°ç»å¸¸å®¹æ˜“å‡ºç°å„ç§bugï¼Œæ‰€ä»¥é¦–è¦ä»»åŠ¡æ˜¯å…ˆç¡®è®¤æˆ‘ä»¬è®¡ç®—çš„ç»“æœæ˜¯æ­£ç¡®çš„ï¼š

```julia
julia> isapprox(dot5_1(cx, cz), dot(cx, convert(CuArray{Float32}, cz)))
true
```

æ³¨æ„è¿™é‡Œç”¨çš„æ˜¯`isapprox`æ¥åšæ¯”è¾ƒã€‚çœ‹èµ·æ¥æˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯æ­£ç¡®çš„ï¼Œé‚£ä¹ˆå…¶æ€§èƒ½å¦‚ä½•å‘¢ï¼Ÿ

```julia
julia> @benchmark CUDA.@sync dot5_1($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 103 samples with 1 evaluation.
 Range (min â€¦ max):  48.510 ms â€¦  54.895 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     48.514 ms               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   48.606 ms Â± 657.494 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

  â–ˆâ–                                                            
  â–ˆâ–ˆâ–ˆâ–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„ â–„
  48.5 ms       Histogram: log(frequency) by time      50.6 ms <

 Memory estimate: 1.78 KiB, allocs estimate: 29.
```

å‘ƒï¼Œè¿˜ä¸å¦‚å…ˆ`convert`äº†å†è°ƒç”¨è‡ªå¸¦çš„`dot`å‡½æ•°......
é‚£é—®é¢˜å‡ºåœ¨å“ªå‘¢ï¼Ÿå…¶å®ä¸Šé¢çš„æ ¸å‡½æ•°åªç”¨äº†ä¸€ä¸ªçº¿ç¨‹åœ¨è®¡ç®—ï¼Œä½†æ˜¯åœ¨GPUä¸Šæœ‰å¤§é‡çš„çº¿ç¨‹å¯ä¾›è®¡ç®—ï¼Œäºæ˜¯ï¼Œå¯ä»¥é‡‡ç”¨ä¸Šé¢çš„CPUä¸Šå¤šçº¿ç¨‹çš„æ–¹æ³•æ¥è®¡ç®—ï¼š

```julia
function dot5_2(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res)
        index = threadIdx().x
        stride = blockDim().x
        for i in index:stride:length(x)
            @inbounds res[] += x[i] * y[i]
        end
    end
    k = @cuda launch=false kernel(x, y, res)
    config = launch_configuration(k.fun)
    k(x, y, res; threads=min(length(x), config.threads))
    CUDA.@allowscalar res[]
end
```

è¿™é‡Œåœ¨è¿è¡Œæ ¸å‡½æ•°çš„æ—¶å€™ï¼ŒæŒ‡å®šäº†`threads`çš„ä¸ªæ•°ï¼Œåœ¨æ ¸å‡½æ•°å†…éƒ¨çš„`for`å¾ªç¯æŠŠæ•°æ®æ ¹æ®`threads`åˆ‡åˆ†æˆäº†ä¸åŒçš„ç‰‡æ®µï¼Œæ¯ä¸ªthreadè´Ÿè´£è®¡ç®—å„è‡ªçš„ä¸€éƒ¨åˆ†ã€‚
å…ˆéªŒè¯ä¸‹æ­£ç¡®æ€§ï¼š

```julia
julia> isapprox(dot5_2(cx, cz), dot(cx, convert(CuArray{Float32}, cz)))
false
```

ç­‰ç­‰ï¼Œè¿™é‡Œä¼¼ä¹çŠ¯äº†å’Œå‰é¢å¤šçº¿ç¨‹è®¡ç®—æ—¶å€™ä¸€æ ·çš„é”™è¯¯ï¼Œåœ¨å¾€`res`é‡Œç´¯ç§¯æ±‚å’Œçš„æ—¶å€™ï¼Œä¼šå­˜åœ¨é™æ€æ¡ä»¶ã€‚ä»”ç»†è§‚å¯Ÿå¯ä»¥å‘ç°ï¼Œæˆ‘ä»¬ä¸ç”¨æ¯æ¬¡éƒ½å¾€`res`é‡Œå†™å…¥ç»“æœï¼Œåªéœ€è¦åœ¨æ¯ä¸ªçº¿ç¨‹å†…éƒ¨å…ˆè®¡ç®—å®Œï¼Œæœ€åå åŠ ä¸Šå»å³å¯ï¼ŒåŒæ—¶æœ€åè¦åŠ é”ã€‚

```julia
function dot5_3(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res, T)
        index = threadIdx().x
        stride = blockDim().x
        s = zero(T)
        for i in index:stride:length(x)
            @inbounds s += x[i] * y[i]
        end
        CUDA.@atomic res[] += s
        return nothing
    end
    k = @cuda launch=false kernel(x, y, res,T)
    config = launch_configuration(k.fun)
    k(x, y, res, T; threads=min(length(x), config.threads))
    CUDA.@allowscalar res[]
end
```

è¿™é‡Œç”¨äº†`CUDA.@atomic`æ¥ä¿è¯åŸå­æ“ä½œï¼ŒåŒæ ·ï¼Œå…ˆç¡®è®¤è®¡ç®—çš„æ­£ç¡®æ€§ï¼š

```julia
julia> isapprox(dot(cx, cz), dot5_3(cx, cz))
true
```

å†çœ‹ä¸‹é€Ÿåº¦å¦‚ä½•ï¼š

```julia
julia> @benchmark CUDA.@sync dot5_3($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  175.298 Î¼s â€¦ 448.774 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     178.373 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   179.218 Î¼s Â±   4.301 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

    â–â–ƒâ–…â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–â–â–â– â–   â–â–â–â–â– â–â–â–â–â–â–â–              â–ƒ
  â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–…â–…â–‡â–…â–† â–ˆ
  175 Î¼s        Histogram: log(frequency) by time        191 Î¼s <

 Memory estimate: 2.16 KiB, allocs estimate: 39.
```

è¿˜ä¸é”™ï¼Œè‡³å°‘æ¯”CPUç‰ˆæœ¬å¿«äº†ï¼Œä½†æ˜¯ç¦»CUBLASç‰ˆæœ¬çš„æ€§èƒ½è¿˜æœ‰ä¸€å®šå·®è·ã€‚

è€ƒè™‘åˆ°ä¸€å—GPUä¸­ï¼Œè¿˜ä¼šæœ‰å¤šä¸ªblockï¼Œè€Œä¸Šé¢æˆ‘ä»¬æ‰ç”¨äº†å…¶ä¸­çš„ä¸€ä¸ªblockï¼Œæ˜¾ç„¶è¿˜æœ‰å¾ˆå¤§çš„ä¼˜åŒ–ç©ºé—´ï¼

ä¸€ä¸ªåŸºæœ¬æ€è·¯æ˜¯ï¼Œæ ¹æ®è¾“å…¥çš„æ•°æ®ï¼Œåˆ†é…å¤šä¸ªblockï¼Œåœ¨æ¯ä¸ªblockçš„æ•°æ®åŒºå—ä¸­ï¼ŒæŒ‰threadå†åˆ‡åˆ†ä¸€æ¬¡ï¼Œæ¯ä¸ªthreadè®¡ç®—è‡ªå·±æ‰€å±çš„æ•°æ®çš„ç‚¹ç§¯ä¹‹å’Œã€‚

```julia
function dot5_4(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res, T)
        index = threadIdx().x
        thread_stride = blockDim().x
        block_stride = (length(x)-1) Ã· gridDim().x + 1
        start = (blockIdx().x - 1) * block_stride + 1
        stop = blockIdx().x * block_stride

        s = zero(T)
        for i in start-1+index:thread_stride:stop
            @inbounds s += x[i] * y[i]
        end
        CUDA.@atomic res[] += s
        return nothing
    end
    k = @cuda launch=false kernel(x, y, res,T)
    config = launch_configuration(k.fun)
    k(x, y, res, T; threads=min(length(x), config.threads), blocks=config.blocks)
    CUDA.@allowscalar res[]
end
```

```julia
julia> isapprox(dot(cx, cz), dot5_4(cx, cz))
true

julia> @benchmark CUDA.@sync dot5_4($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  134.011 Î¼s â€¦ 383.172 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     134.933 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   135.095 Î¼s Â±   2.723 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

       â–â–ƒâ–„â–‡â–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ƒâ–ƒâ–                                          
  â–‚â–‚â–ƒâ–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚ â–„
  134 Î¼s           Histogram: frequency by time          138 Î¼s <

 Memory estimate: 2.16 KiB, allocs estimate: 39.
```

OK, çœ‹èµ·æ¥ç¨å¾®å¿«äº†ä¸€äº›ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå‰é¢æˆ‘ä»¬ç›´æ¥å°†æ¯ä¸ªthreadè®¡ç®—çš„ç»“æœå¾€ä¸€ä¸ª`res`å¯¹è±¡ä¸­é€šè¿‡åŠ é”å åŠ ä¸Šå»äº†ï¼Œè¿™æ ·å¯¼è‡´æ¯ä¸ªblockä¸­æ¯ä¸ªthreadéƒ½ä¼šå¡åœ¨åŸå­æ“ä½œé‚£ä¸€æ­¥ã€‚
ä¸€ç§ä¼˜åŒ–æ–¹å¼æ˜¯æ¯ä¸ªblockçš„å†…éƒ¨ï¼Œå…ˆæŠŠå„ä¸ªthreadçš„è®¡ç®—ç»“æœç¼“å­˜èµ·æ¥ï¼Œç­‰ä¸€ä¸ªblockå†…æ‰€æœ‰threadéƒ½è®¡ç®—å‡ºæ¥äº†åŒæ­¥ä¸€ä¸‹ï¼Œç„¶åå†…éƒ¨å…ˆreduceï¼Œæœ€åå†é€šè¿‡åŸå­æ“ä½œåŒæ­¥åˆ°æœ€ç»ˆçš„ç»“æœä¸Šã€‚

```julia
function dot5_5(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res, T)
        index = threadIdx().x
        thread_stride = blockDim().x
        block_stride = (length(x)-1) Ã· gridDim().x + 1
        start = (blockIdx().x - 1) * block_stride + 1
        stop = blockIdx().x * block_stride

        cache = CuDynamicSharedArray(T, (thread_stride,))

        for i in start-1+index:thread_stride:stop
            @inbounds cache[index] += x[i] * y[i]
        end

        sync_threads()

        if index == 1
            s = zero(T)
            for i in 1:thread_stride
                s += cache[i]
            end
            CUDA.@atomic res[] += s
        end
        return nothing
    end
    k = @cuda launch=false kernel(x, y, res,T)
    config = launch_configuration(k.fun; shmem=(threads) -> threads*sizeof(T))
    threads = min(length(x), config.threads)
    blocks = config.blocks
    k(x, y, res, T; threads=threads, blocks=config.blocks, shmem=threads*sizeof(T))
    CUDA.@allowscalar res[]
end
```

```julia
julia> isapprox(dot(cx, cz), dot5_5(cx, cz))
true

julia> @benchmark CUDA.@sync dot5_5($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  54.364 Î¼s â€¦ 358.597 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     55.217 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   55.559 Î¼s Â±   4.023 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

     â–„â–‡â–ˆâ–‡â–…â–ƒâ–‚                                                    
  â–‚â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚ â–ƒ
  54.4 Î¼s         Histogram: frequency by time         61.3 Î¼s <

 Memory estimate: 2.33 KiB, allocs estimate: 43.
```

å¯ä»¥çœ‹åˆ°,å…¶æ€§èƒ½è·ŸCUBLASæ¯”è¾ƒæ¥è¿‘äº†ã€‚å½“ç„¶ï¼Œä¸Šé¢çš„ä»£ç è¿˜å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¸Šé¢æœ€åreduceçš„æ—¶å€™ï¼Œåªæœ‰indexä¸º1çš„çº¿ç¨‹åœ¨è¿è¡Œï¼Œå…¶å®å¯ä»¥å¤šä¸ªçº¿ç¨‹ä¸€èµ·å·¥ä½œï¼š

```julia
using CUDA:i32

function dot5_6(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res, T)
        index = threadIdx().x
        thread_stride = blockDim().x
        block_stride = (length(x)-1i32) Ã· gridDim().x + 1i32
        start = (blockIdx().x - 1i32) * block_stride + 1i32
        stop = blockIdx().x * block_stride

        cache = CuDynamicSharedArray(T, (thread_stride,))

        for i in start-1i32+index:thread_stride:stop
            @inbounds cache[index] += x[i] * y[i]
        end
        sync_threads()

        mid = thread_stride
        while true
            mid = (mid - 1i32) Ã· 2i32 + 1i32
            if index <= mid
                @inbounds cache[index] += cache[index+mid]
            end
            sync_threads()
            mid == 1i32 && break
        end

        if index == 1i32
            CUDA.@atomic res[] += cache[1]
        end
        return nothing
    end
    k = @cuda launch=false kernel(x, y, res,T)
    config = launch_configuration(k.fun; shmem=(threads) -> threads*sizeof(T))
    threads = min(length(x), config.threads)
    blocks = config.blocks
    k(x, y, res, T; threads=threads, blocks=config.blocks, shmem=threads*sizeof(T))
    CUDA.@allowscalar res[]
end
```

```julia
julia> isapprox(dot(cx, cz), dot5_6(cx, cz))
true

julia> @benchmark CUDA.@sync dot5_6($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  22.520 Î¼s â€¦ 375.954 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     23.475 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   23.762 Î¼s Â±   3.748 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

        â–â–…â–ˆâ–ˆâ–†â–ƒâ–                                                 
  â–‚â–‚â–ƒâ–ƒâ–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–„â–…â–…â–…â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚ â–ƒ
  22.5 Î¼s         Histogram: frequency by time           28 Î¼s <

 Memory estimate: 2.16 KiB, allocs estimate: 39.
```

è¿™æ ·ï¼Œæœ€ç»ˆçš„ç»“æœè·ŸCUBLASçš„æ€§èƒ½åŸºæœ¬ä¸€è‡´äº†ã€‚

ä»ä»£ç å±‚é¢ä¸Šè®²ï¼Œä¸Šé¢çš„ä»£ç è¿˜å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ä¸‹ï¼Œä¸Šé¢çš„whileå¾ªç¯å…¶å®æ˜¯ä¸€ä¸ªç»å…¸çš„reduceæ“ä½œï¼Œè€Œ`CUDA.jl`ä¸­å†…ç½®äº†ä¸€ä¸ªå‡½æ•°`reduce_block`æ¥ç®€åŒ–è¯¥æ“ä½œ:

```julia
function dot5_7(x::CuArray{T1}, y::CuArray{T2}) where {T1, T2}
    T = promote_type(T1, T2)
    res = CuArray{T}([zero(T)])
    function kernel(x, y, res, T)
        index = threadIdx().x
        thread_stride = blockDim().x
        block_stride = (length(x)-1i32) Ã· gridDim().x + 1i32
        start = (blockIdx().x - 1i32) * block_stride + 1i32
        stop = blockIdx().x * block_stride

        local_val = zero(T)

        for i in start-1i32+index:thread_stride:stop
            @inbounds local_val += x[i] * y[i]
        end

        val = CUDA.reduce_block(+, local_val, zero(T), #=shuffle=# Val(true))
        if threadIdx().x == 1i32
            @inbounds CUDA.@atomic res[] += val
        end
        return
    end
    k = @cuda launch=false kernel(x, y, res,T)
    config = launch_configuration(k.fun; shmem=(threads) -> threads*sizeof(T))
    threads = min(length(x), config.threads)
    blocks = config.blocks
    k(x, y, res, T; threads=threads, blocks=config.blocks, shmem=threads*sizeof(T))
    CUDA.@allowscalar res[]
end
```

```julia
julia> @benchmark CUDA.@sync dot5_7($(cu(rand(N))), $(cu(rand(Bool, N))))
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min â€¦ max):  23.674 Î¼s â€¦ 252.995 Î¼s  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     25.095 Î¼s               â”Š GC (median):    0.00%
 Time  (mean Â± Ïƒ):   25.911 Î¼s Â±   3.444 Î¼s  â”Š GC (mean Â± Ïƒ):  0.00% Â± 0.00%

      â–â–‡â–ˆâ–„â–  â–                                                  
  â–â–ƒâ–„â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–‡â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â– â–‚
  23.7 Î¼s         Histogram: frequency by time         33.2 Î¼s <

 Memory estimate: 2.33 KiB, allocs estimate: 43.
```

## å‚è€ƒ

- [Introduction to CUDA.jl](https://cuda.juliagpu.org/stable/tutorials/introduction/)
- [GTC-2010](https://www.nvidia.com/content/GTC-2010/pdfs/2131_GTC2010.pdf)
- [CUDA.jl#1240](https://github.com/JuliaGPU/CUDA.jl/pull/1240)

```@comment
```