<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating] Â· Jun Tian</title><meta name="title" content="å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating] Â· Jun Tian"/><meta property="og:title" content="å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating] Â· Jun Tian"/><meta property="twitter:title" content="å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating] Â· Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">ğŸ‘‹ About</a></li><li><span class="tocitem">ğŸ’» Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">ğŸ“– Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">ğŸ™‹ Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">ğŸ”— Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ï‚›</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/Reinforcement_Learning_in_Action/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ï„</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: Algorithm,Julia,ReinforcementLearning CJKmainfont: KaiTi â€“-</p><h1 id="å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]"><a class="docs-heading-anchor" href="#å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]">å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]</a><a id="å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]-1"></a><a class="docs-heading-anchor-permalink" href="#å¼ºåŒ–å­¦ä¹ å®æˆ˜[updating]" title="Permalink"></a></h1><p>[ç­‰æˆ‘ç”¨Juliaå†™ä¸ªRLçš„åº“äº†å†æ¥ç»§ç»­æ›´æ–°ã€‚]</p><p>æˆ‘ä¸€ç›´å¸Œæœ›æ‰¾åˆ°ä¸€ç¯‡é•¿æ–‡ï¼Œèƒ½ä»å®ç”¨çš„è§’åº¦è®²æ¸…æ¥šå¼ºåŒ–å­¦ä¹ ã€‚ä¸€å¹´è¿‡å»äº†ï¼Œæˆ‘å†³å®šè‡ªå·±åŠ¨æ‰‹å†™è¿™æ ·ä¸€ç¯‡æ–‡ç« ã€‚</p><h2 id="ç¼–ç¨‹è¯­è¨€"><a class="docs-heading-anchor" href="#ç¼–ç¨‹è¯­è¨€">ç¼–ç¨‹è¯­è¨€</a><a id="ç¼–ç¨‹è¯­è¨€-1"></a><a class="docs-heading-anchor-permalink" href="#ç¼–ç¨‹è¯­è¨€" title="Permalink"></a></h2><p>æœ¬æ–‡å°†ä½¿ç”¨Juliaä½œä¸ºä¸»è¦çš„ç¼–ç¨‹è¯­è¨€æ¥å®ç°Reinforcement Learningä¸­çš„å¤§å¤šæ•°ç®—æ³•ã€‚</p><p>Why Julia?äº‹å®ä¸Šï¼Œæˆ‘æ›´ä¹æ„ä½¿ç”¨Clojureï¼Œç„¶è€Œå…¶åŸºç¡€å·¥å…·åº“å®åœ¨å¤ªåŒ®ä¹ï¼Œä¸å¾—ä¸æ”¾å¼ƒã€‚Why not Python?æˆ‘åªæ˜¯å•çº¯è§‰å¾—Juliaå†™å‡ºæ¥çš„ä»£ç æ›´å¥½çœ‹ï¼ä¸è¿‡é˜…è¯»æœ¬æ–‡å¹¶ä¸éœ€è¦è¯»è€…ç†Ÿç»ƒæŒæ¡Juliaï¼Œåªéœ€è¦æœ‰åŸºæœ¬çš„ç¼–ç¨‹æ€æƒ³å³å¯ï¼Œæˆ‘ä¼šå°½é‡å°†Juliaè¿™é—¨è¯­è¨€ç‹¬æœ‰çš„å†…å®¹é™åˆ°æœ€ä½ï¼ˆå¿…è¦çš„æ—¶å€™ä¼šç»™å‡ºè§£é‡Šå’Œå‚è€ƒæ–‡çŒ®ï¼‰ã€‚</p><h1 id="1.-Tabular-Methods"><a class="docs-heading-anchor" href="#1.-Tabular-Methods">1. Tabular Methods</a><a id="1.-Tabular-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Tabular-Methods" title="Permalink"></a></h1><blockquote><p>â€œæ˜¨å¤œè¥¿é£å‡‹ç¢§æ ‘ï¼Œç‹¬ä¸Šé«˜æ¥¼ï¼Œæœ›å°½å¤©æ¶¯è·¯ã€‚â€</p></blockquote><h2 id="1.1-Day-1:-Where&#39;s-Eve?"><a class="docs-heading-anchor" href="#1.1-Day-1:-Where&#39;s-Eve?">1.1 Day 1: Where&#39;s Eve?</a><a id="1.1-Day-1:-Where&#39;s-Eve?-1"></a><a class="docs-heading-anchor-permalink" href="#1.1-Day-1:-Where&#39;s-Eve?" title="Permalink"></a></h2><p><img src="img/eve_move.png" alt="eve_move"/></p><p>å‡è®¾æŸä¸ªè™šæ‹Ÿçš„ä¸–ç•Œä¸­åªæœ‰ä¸¤ä¸ªæ˜Ÿçƒï¼ˆé»„è‰²æ˜Ÿçƒ[Y]å’Œçº¢è‰²æ˜Ÿçƒ[R]ï¼‰ï¼Œæ¯ä¸ªæ˜Ÿçƒä¸Šå„æœ‰ä¸€æšç¡¬å¸ï¼ˆæ­£é¢æœä¸Šçš„æ¦‚ç‡æœ¬åˆ«æ˜¯<span>$p$</span>, <span>$q$</span>ï¼‰ï¼Œæˆ‘ä»¬çš„æœºå™¨äººæœ‹å‹Eveçš„åˆå§‹ä½ç½®åœ¨å·¦è¾¹çš„é»„è‰²æ˜Ÿçƒä¸Š(<span>$s_0=Y$</span>)ï¼Œæ¥ä¸‹æ¥ï¼Œå®ƒæ¯å¤©éƒ½å°è¯•æŠ•æ·å½“å‰æ‰€åœ¨æ˜Ÿçƒä¸Šçš„ç¡¬å¸ï¼Œå¦‚æœæ­£é¢æœä¸Šï¼Œé‚£ä¹ˆå®ƒç§»åŠ¨åˆ°å¦å¤–ä¸€ä¸ªæ˜Ÿçƒä¸Šï¼Œå¦åˆ™å‘†åœ¨åŸåœ°ä¸åŠ¨ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºç¬¬<span>$t$</span>å¤©Eveå¤„äºé»„è‰²æ˜Ÿçƒçš„æ¦‚ç‡:<span>$P(s_t=Y)=P(s_{t-1}=Y)(1-p) + P(s_{t-1}=R)(q)$</span>ï¼Œç±»ä¼¼çš„ï¼Œå¤„äºçº¢è‰²æ˜Ÿçƒçš„æ¦‚ç‡ä¸ºï¼š<span>$P(s_t=R) = P(s_{t-1}=Y)(p) + P(X_{t-1}=R)(1-q)$</span>.å°†å…¶å†™æˆæ¦‚ç‡è½¬ç§»çŸ©é˜µçš„å½¢å¼å¦‚ä¸‹ï¼š</p><p>$</p><p>\begin{equation} \boldsymbol{P} = \left( \begin{matrix} 1-p &amp; p \
q &amp; 1-q \end{matrix} \right) \end{equation} $</p><p>äºæ˜¯ï¼Œ<span>$t$</span>æ—¶åˆ»Eveæ‰€å¤„çŠ¶æ€çš„æ¦‚ç‡ä¸ºï¼š</p><p>$</p><p>\begin{equation} \boldsymbol{s}<em>t = \boldsymbol{s}</em>0 \boldsymbol{P}^t \end{equation} $ è¿™é‡Œï¼Œ<span>$\boldsymbol{s}_0=(1,0)$</span>ï¼Œå³åˆå§‹çŠ¶æ€å¤„äºå·¦è¾¹çš„é»„è‰²æ˜Ÿçƒã€‚ç”±äºè¯¥æ¦‚ç‡è½¬ç§»çŸ©é˜µçš„ç‰¹æ®Šæ€§è´¨ï¼Œ<span>$\boldsymbol{s}_t$</span>æœ€ç»ˆä¼šæ”¶æ•›åˆ°ä¸€ä¸ªç¨³æ€ã€‚ä¸‹å›¾ä¸º<span>$(p,q)$</span>åˆ†åˆ«å–(0.5,0.5),(0.2, 0.1),(0.95,0.7)æ—¶ï¼Œ<span>$\boldsymbol{s}_t$</span>çš„æ”¶æ•›è¿‡ç¨‹ï¼š</p><p><img src="img/eve_planet.png" alt="eve_planet"/></p><p>å‡è®¾æˆ‘ä»¬çš„æœºå™¨äººEveè½åœ¨äº†ä¸€æ¡æ•°è½´çš„åŸç‚¹å¤„ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯å¯»æ‰¾åˆ°ç”Ÿå‘½ï¼Œåœ¨-3å’Œ5å¤„åˆ†åˆ«æœ‰ä¸€ç›†èŠ±å’Œä¸€åªå°ç‹—ï¼ŒEveæ¯å¤©åªèƒ½é€‰æ‹©å‘å·¦æˆ–å‘å³ç§»åŠ¨ä¸€ä¸ªå•ä½ï¼Œåªè¦å‘ç°ç”Ÿå‘½ï¼ˆèŠ±æˆ–è€…ç‹—ï¼‰åï¼Œä»»åŠ¡ä¾¿ç»“æŸï¼Œç„¶åEveè¿”å›å¹¶è·å¾—ç›¸åº”çš„å¥–åŠ±ï¼ˆå‡è®¾å¥–åŠ±åˆ†åˆ«æ˜¯3å’Œ5ï¼Œå…¶å®ƒä½ç½®æ²¡æœ‰å¥–åŠ±ï¼‰ã€‚ æ˜¾ç„¶ï¼Œå¯¹äºè¿™æ ·ä¸€ä¸ªç®€å•è€Œä¸”ç¡®å®šçš„é—®é¢˜è€Œè¨€ï¼Œæœ‰å¤šç§æœç´¢çš„æ–¹æ³•å¾—åˆ°è§£ï¼Œè¿™é‡Œå…ˆå‡è®¾Eveå¹¶ä¸çŸ¥é“ç¯å¢ƒæ˜¯ç¡®å®šçš„ã€‚</p><p><img src="img/Eve_Example.png" alt="Eve_Example"/></p><p>ç°åœ¨å…ˆç»™å‡ºå¦‚ä¸‹å®šä¹‰ï¼š</p><ul><li><p class="math-container">\[\mathcal{A}\]</p>ï¼šActions, <strong>åŠ¨ä½œç©ºé—´</strong>,åœ¨è¿™é‡ŒåŒ…å«ä¸¤ç§å¯èƒ½<span>${:left, :right}$</span></li><li><p class="math-container">\[a_t\]</p>: åœ¨<span>$t$</span>æ—¶åˆ»é‡‡å–çš„è¡ŒåŠ¨</li><li><p class="math-container">\[\mathcal{S}\]</p>: States, <strong>çŠ¶æ€ç©ºé—´</strong>ï¼Œè¿™é‡Œå°±æ˜¯Eveæ‰€æœ‰å¯èƒ½çš„ä½ç½®<span>${-3, -2, -1, 0, 1, 2, 3, 4, 5}$</span></li><li><p class="math-container">\[s_t\]</p>: åœ¨<span>$t$</span>æ—¶åˆ»æ‰€å¤„çš„çŠ¶æ€</li><li><p class="math-container">\[\mathcal{R}\]</p>:æ‰€æœ‰å¯èƒ½çš„å¥–åŠ±ï¼Œè¿™é‡Œç”±ä¸‰ä¸ªç¦»æ•£å€¼<span>${0, 3, 5}$</span>æ„æˆ</li><li><p class="math-container">\[R_t\]</p>ï¼šæ¯ä¸€å¤©è·å¾—çš„å¥–åŠ±ï¼Œç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å°†æ¸¸æˆç»“æŸæ—¶çš„æ—¶é—´è®°ä¸º<span>$T$</span>ï¼Œä»»åŠ¡ç»“æŸæ—¶è·å¾—çš„å¥–åŠ±ä¸º<span>$R_T$</span></li></ul><p>å‡è®¾Eveå¾ˆç¬¨ï¼Œå®ƒçš„è®°å¿†åªæœ‰ä¸€å¤©ï¼ˆå¹¶ä¸è®°å¾—å®ƒæ›¾èµ°è¿‡å“ªäº›åœ°æ–¹ï¼‰ï¼Œæ¯å¤©åªä¼šç­‰æ¦‚ç‡åœ°éšæœºé€‰æ‹©å‘å·¦æˆ–å‘å³ç§»åŠ¨ï¼Œå°†å…¶è®°ä½œ<strong>ç­–ç•¥</strong><span>$\pi$</span>ï¼ˆå°½ç®¡ç›®å‰è¿˜åªæ˜¯éšæœºæ¸¸èµ°ï¼Œè°ˆä¸ä¸Šä»€ä¹ˆç­–ç•¥ï¼‰ã€‚æˆ‘ä»¬å…ˆè§‚å¯Ÿä»¥ä¸‹ä¸¤ä¸ªæŒ‡æ ‡ï¼š</p><ol><li><p class="math-container">\[\bar{R_T} = \frac{\sum_{i=1}^{N} R_T^i}{N}\]</p>ï¼ŒNæ¬¡è¯•éªŒçš„å¹³å‡æ”¶ç›Š(<span>$3.746$</span>)</li></ol><p><img src="img/eve_reward_count.png" alt="ä»»åŠ¡ç»“æŸæ—¶è·å¾—çš„å¥–åŠ±åˆ†å¸ƒ"/></p><ol><li><p class="math-container">\[\bar{T} = \frac{\sum_{i=1}^{N} T^i}{N}\]</p>ï¼ŒNæ¬¡è¯•éªŒå¹³å‡è¡ŒåŠ¨çš„æ¬¡æ•°(<span>$16.164$</span>)</li></ol><p><img src="img/eve_steps_count.png" alt="ä»»åŠ¡ç»“æŸæ—¶ï¼Œè¡ŒåŠ¨æ¬¡æ•°çš„åˆ†å¸ƒ"/></p><p>å¹³å‡æ”¶ç›Šåªæœ‰3.7å·¦å³ï¼Œç¦»æœ€ä¼˜è§£ï¼ˆ5ï¼‰è¿˜æœ‰ç‚¹è·ç¦»ï¼Œæ­¤å¤–å¹³å‡å®éªŒæ¬¡æ•°å±…ç„¶åˆ°äº†16æ¬¡ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸€æ­¥æ­¥æ”¾æ¾çº¦æŸæ¡ä»¶ï¼Œæ”¹è¿›Eveçš„è¡ŒåŠ¨ç­–ç•¥ã€‚</p><p>è¿™é‡Œå…ˆå°†å‰é¢Eveä¸ç¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ç”¨ä¸‹å›¾æŠ½è±¡å‡ºæ¥ï¼š</p><p><img src="img/MDP_general_representation.png" alt="MDPçš„åºåˆ—åŒ–æè¿°"/></p><p>æ¯ä¸€æ­¥çš„rewardåªç”±å½“å‰æ—¶åˆ»çš„stateå’Œactionå…±åŒå†³å®šï¼ˆè¿™é‡Œå¯¹äºEveæ¥è¯´ï¼Œæ¯å¤©çš„rewardå…¶å®åªä¸stateç›¸å…³ï¼Œç¯å¢ƒå¹¶ä¸å—Eveçš„è¡ŒåŠ¨å½±å“ï¼Œå½“ç„¶ï¼ŒçœŸå®æƒ…å†µè¦æ¯”è¿™å¤æ‚å¾—å¤šï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼‰ï¼Œè€Œä¸‹ä¸€æ—¶åˆ»çš„stateåˆ™åªå—ä¸Šä¸€ä¸ªæ—¶åˆ»çš„stateå’Œactionå…±åŒå†³å®šã€‚</p><h2 id="1.2-Day-2:-The-Value-of-State"><a class="docs-heading-anchor" href="#1.2-Day-2:-The-Value-of-State">1.2 Day 2: The Value of State</a><a id="1.2-Day-2:-The-Value-of-State-1"></a><a class="docs-heading-anchor-permalink" href="#1.2-Day-2:-The-Value-of-State" title="Permalink"></a></h2><p>æŸä¸€å¤©ï¼ŒEveæ„Ÿåˆ°å¾ˆè¿·èŒ«ï¼Œå®ƒä¸çŸ¥é“æœªæ¥è¿™ä¹ˆèµ°ä¸‹å»ï¼Œæ”¶ç›Šç©¶ç«Ÿæœ‰å¤šå°‘ï¼Œæˆ‘ä»¬å¯ä»¥ç»™Eveç®—ä¸€ä¸‹æœªæ¥æ”¶ç›Šçš„æ€»å’Œ<span>$G$</span>ï¼š</p><p>$</p><p>\begin{equation} G<em>t = R</em>t + R<em>{t+1} + R</em>{t+2} ... \end{equation} $</p><p>ä¸è¿‡ï¼ŒEveå¯èƒ½å¹¶ä¸è¿™ä¹ˆè®¤ä¸ºï¼ŒåŒæ ·çš„æ”¶ç›Šï¼Œç¬¬5å¤©å¾—åˆ°è¿˜æ˜¯ç¬¬3å¤©å¾—åˆ°å¯¹äºEveæ¥è¯´æœ‰ç€ä¸åŒçš„æ„ä¹‰ï¼ˆæ˜¾ç„¶åè€…çš„æ„ä¹‰æ›´å¤§ï¼‰ï¼Œæ‰€ä»¥ä¸å¦¨ç»™æ¯å¤©çš„æ”¶ç›Šå¢åŠ ä¸€ä¸ªåŸºäºæ—¶é—´çš„æŠ˜æ‰£ç³»æ•°<span>$\gamma$</span>ï¼ˆè¿™ä¹ˆåšæœ‰è®¸å¤šå¥½å¤„ï¼Œæ•°å­¦ä¸Šå¤„ç†èµ·æ¥æ›´æ–¹ä¾¿ï¼Œè€Œä¸”å¯¹äºinfiniteçš„é—®é¢˜ä¹Ÿæ›´å®¹æ˜“æ±‚è§£ï¼Œå½“ç„¶ï¼Œæ ¹æ®å®é™…é—®é¢˜ä¸åŒï¼Œä½ å®Œå…¨å¯ä»¥è®¾è®¡ä¸åŒçš„<span>$G_t$</span>è®¡ç®—æ–¹å¼ï¼Œæ¯”å¦‚averaged rewardï¼‰ã€‚</p><p>$</p><p>\begin{equation} G<em>t = R</em>t + \gamma R<em>{t+1} + \gamma^2 R</em>{t+2} + ... \end{equation} $</p><p>é‚£ä¹ˆï¼Œæ ¹æ®Eveåœ¨<span>$t$</span>æ—¶åˆ»æ‰€å¤„çš„çŠ¶æ€ä¸åŒï¼Œå‡è®¾å¯ä»¥ç®—å‡ºæ”¶ç›Šçš„æœŸæœ›ï¼Œç§°ä½œvalue function:</p><p>$</p><p>\begin{equation} V<em>t(s) = \mathbb{E}(G</em>t | s) = \mathbb{E}(R<em>t + \gamma R</em>{t+1} + \gamma^2 R<em>{t+2} + ... | s) \label{value</em>equation} \end{equation} $ ä»ä¸Šå¸è§†è§’æ¥çœ‹ï¼Œåœ¨ç¯å¢ƒä¸å‘ç”Ÿå˜åŒ–çš„æƒ…å†µä¸‹ï¼Œæ˜¾ç„¶æ¯ä¸ªçŠ¶æ€séƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„å›ºå®šçš„<span>$V(s)$</span>ï¼Œè¿™æ ·ï¼Œæ¯æ¬¡Eveå†³å®šæ€ä¹ˆèµ°çš„æ—¶å€™ï¼Œåªéœ€è¦çœ‹ä¸‹è¿™å¼ è¡¨ï¼Œä»å¯ä»¥åˆ°è¾¾çš„æ‰€æœ‰çŠ¶æ€ä¸­ï¼Œæ‰¾åˆ°ä»·å€¼æœ€é«˜çš„çŠ¶æ€<span>$s&#39;$</span>ï¼Œè·³è½¬è¿‡å»å³å¯ã€‚ç°åœ¨é—®é¢˜å˜æˆäº†å¦‚ä½•è®¡ç®—<span>$V(s)$</span>ã€‚æˆ‘ä»¬ä¸å¦¨å…ˆæ¨¡æ‹Ÿä¸‹ï¼Œå°†Eveæ”¾åœ¨ä¸åŒçš„ä½ç½®ï¼Œç„¶ååˆ†åˆ«æ‰§è¡ŒåŸæ¥çš„éšæœºç­–ç•¥ï¼Œè®¡ç®—å¹³å‡æ”¶ç›Š<span>$\hat{G_t}$</span>ï¼Œæ¥ä¼°è®¡<span>$V(s)$</span>ã€‚</p><p><img src="img/eve_v_estimate.png" alt="éšæœºæ¨¡æ‹Ÿä¼°è®¡V(s)ï¼Œgamma=1.0"/></p><p>çœ‹èµ·æ¥ä¸Šå›¾ä¼¼ä¹ç¬¦åˆæˆ‘ä»¬çš„ç›´è§‚æ„Ÿå—ã€‚<span>$\gamma$</span>è¶Šå¤§ï¼Œæˆ‘ä»¬å°±è¶Šæ˜¯çœ‹é‡é•¿æœŸæ”¶ç›Šï¼Œåä¹‹åˆ™æ›´çœ‹é‡çŸ­æœŸæ”¶ç›Šã€‚</p><p>ä¸è¿‡ï¼Œå¯¹æˆ‘ä»¬çš„æœºå™¨äººEveæ¥è¯´ï¼Œæ‰‹ä¸Šå¹¶æ²¡æœ‰è¿™æ ·ä¸€å¼ è¡¨ï¼Œå‰é¢è™½ç„¶é€šè¿‡æ¨¡æ‹Ÿå¾—åˆ°äº†è¿™æ ·ä¸€å¼ è¡¨ï¼Œä½†æ˜¯æ•ˆç‡è¿˜å¾ˆä½ã€‚é‚£æ˜¯å¦èƒ½æ‰¾åˆ°è§£æè§£å‘¢ï¼Ÿé¦–å…ˆå¼•å…¥çŠ¶æ€è½¬ç§»çš„æ¦‚å¿µï¼Œè¿™é‡ŒEveæ¯ä¸€æ—¶åˆ»æ‰€å¤„çš„çŠ¶æ€åªä¸å…¶ä¸Šä¸€æ—¶å€™æ‰€å¤„çš„çŠ¶æ€ï¼ˆå’Œè¡ŒåŠ¨ï¼‰æœ‰å…³ï¼Œä¸å—æ›´æ—©æ—¶å€™çŠ¶æ€ï¼ˆå’Œè¡Œä¸ºï¼‰çš„å½±å“ï¼ˆå³å…·æœ‰Markovæ€§è´¨ï¼‰ï¼Œäºæ˜¯æœ‰ï¼š</p><p>$</p><p>\begin{equation} P(s&#39;|s) = \sum_{a \in A} P(s&#39; | s, a) \end{equation} $</p><p>å¯¹äºå‰é¢Eveæœºå™¨äººéšæœºç­–ç•¥è€Œè¨€ï¼Œå¯¹åº”çš„æ¦‚ç‡è½¬ç§»çŸ©é˜µä¸ºï¼š</p><p>$</p><p>\begin{equation} \begin{bmatrix} 1.0  &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0         \
0.5 &amp;0 &amp; 0.5  &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0      \
0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0      \
0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0 &amp;0     \
0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0    \
0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5  &amp;0 &amp;0  \
0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0  \
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 \
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0 &amp; 1.0 \
\end{bmatrix} \end{equation} $</p><p>äºæ˜¯ï¼Œ<span>$\eqref{value_equation}$</span>å¯ä»¥å†™æˆï¼š</p><p>$</p><p>\begin{equation} V<em>t(s) = R</em>t(s) + \gamma \sum<em>{s&#39; \in S} P(s&#39; | s) V</em>{t+1}(s&#39;) \end{equation} $</p><p>ä¸Šå¼æ„å»ºäº†ä¸åŒæ—¶åˆ»çš„ä»·å€¼å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œåœ¨è¿™é‡Œç”±äºçŠ¶æ€è½¬ç§»å‡½æ•°æ˜¯å›ºå®šçš„ï¼Œæœ€ç»ˆä¼šè¾¾åˆ°ä¸€ä¸ªç¨³æ€ï¼ˆTODO: è¿™é‡Œéœ€è¦å±•å¼€è¯´ä¸‹ï¼ŒMRPï¼‰ï¼Œå³ï¼š</p><p>$</p><p>\begin{equation} \begin{bmatrix} V^<em>(s_1) \
\dots \
V^</em>(s<em>N)  \end{bmatrix} =  \begin{bmatrix} R(s</em>1) \
\dots \
R(s<em>N)  \end{bmatrix} + \gamma \begin{bmatrix} P(s</em>1|s<em>1) &amp; \dots &amp;P(s</em>N|s<em>1)\
\vdots  &amp;\ddots &amp;\vdots \
P(s</em>1|s<em>N) &amp;\dots &amp;P(s</em>N|s<em>N)  \end{bmatrix} \begin{bmatrix} V^*(s</em>1) \
\dots \
V^*(s_N)  \end{bmatrix} \end{equation} $</p><p>é‡‡ç”¨çŸ©é˜µçš„è¡¨ç¤ºæ–¹æ³•å³æ˜¯ï¼š</p><p>$</p><p>\begin{equation} \begin{split} \boldsymbol{V} &amp;= \boldsymbol{R} + \gamma \boldsymbol{P V}\
\boldsymbol{V} &amp;= (\boldsymbol{I} - \gamma \boldsymbol{P})^{-1} \boldsymbol{R} \end{split} \end{equation} $</p><pre><code class="language-julia hljs">P = diagm(repmat([0.5], 8), 1) + diagm(repmat([0.5], 8), -1)
P[1,1], P[1,2], P[end, end-1], P[end, end] = 1, 0, 0, 1
V(Î³) = (diagm(ones(9)) -  Î³*P)^-1 * R
show(V(0.9))
# [30.0, 19.9416, 14.3146, 11.8686, 12.0601, 14.9316, 21.1213, 32.0046, 50.0]
show(V(0.95))
# [60.0, 48.1992, 41.4721, 39.1104, 40.8656, 46.9225, 57.9186, 75.0113, 100.0]
show(V(0.99))
# [300.0, 301.464, 309.018, 322.814, 343.132, 370.383, 405.115, 448.032, 500.0]</code></pre><p>ç¨ å¯†çŸ©é˜µæ±‚é€†çš„å¤æ‚åº¦ä¸º<span>$O(n^3)$</span>ï¼ˆTODO: è¿™é‡Œéœ€è¦æ’å…¥ç›¸å…³æ–‡çŒ®ï¼‰ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«é‡‡ç”¨Value Iterationå’Œ Policy Iterationæ¥æ±‚è§£ã€‚</p><h3 id="Value-Iteration"><a class="docs-heading-anchor" href="#Value-Iteration">Value Iteration</a><a id="Value-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Value-Iteration" title="Permalink"></a></h3><p>è®©æˆ‘ä»¬å…ˆä»ä¸Šå¸è§†è§’æ¥çœ‹çœ‹è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æœ¬æ„æ˜¯å¸Œæœ›å¾—åˆ°ä¸€å¼ è¡¨ï¼Œè®©Eveæ¯æ¬¡è¡ŒåŠ¨çš„æ—¶å€™ï¼Œèƒ½å¤ŸæŸ¥çœ‹å®ƒä»å½“å‰ä½ç½®å¾€å·¦å’Œå¾€å³èµ°çš„æœŸæœ›ä»·å€¼ï¼Œç„¶åä»ä¸­é€‰å‡ºæœŸæœ›ä»·å€¼æœ€å¤§çš„è¡ŒåŠ¨<span>$a$</span>ä½œä¸ºå†³ç­–Policyçš„ç»“æœã€‚æ˜¾ç„¶ï¼Œæœ€ç³Ÿç³•çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦éå†æ‰€æœ‰çš„å¯èƒ½<span>$|A|^{|S|}$</span>ï¼Œä¸è¿‡é€šè¿‡Value Iterationï¼Œæˆ‘ä»¬å¯ä»¥å…ˆæ‰¾åˆ°ç¨³æ€çš„<span>$V(s)$</span>ï¼Œç„¶åç›´æ¥å¾—å‡ºæœ€ä¼˜çš„Policyã€‚å¯ä»¥è¯æ˜åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹ï¼ˆTODO: Reference Needed Here, Bellman Functionï¼‰ï¼Œä»·å€¼å‡½æ•°æ»¡è¶³ä»¥ä¸‹å¼å­ï¼š</p><p>$</p><p>\begin{equation} V^<em>(s) = \underset{a \in A}{max} \left( R(s, a) + \gamma \sum_{s&#39; \in S}P(s&#39; | s,a) V^</em>(s&#39;) \right) \end{equation} $</p><p>äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆéšæœºåˆå§‹åŒ–<span>$V(s)$</span>ï¼Œç„¶åæ ¹æ®ä¸Šå¼è¿­ä»£è®¡ç®—å¹¶æ›´æ–°ï¼Œç›´è‡³æ”¶æ•›ã€‚ä¸‹å›¾åŠ¨æ€åœ°å±•ç¤ºäº†è¯¥è¿‡ç¨‹ï¼š</p><p><img src="img/eve_v_iteration_gamma_0.9.gif" alt="gamma=0.9æ—¶ï¼ŒValue è¿­ä»£çš„è¿‡ç¨‹"/></p><p>ä¸åŒgammaå¯¹åº”çš„ç¨³æ€Value:</p><p><img src="img/eve_v_iteration.png" alt="ä¸åŒgammaå¯¹åº”çš„ç¨³æ€Value"/></p><p>åœ¨å¾—åˆ°ç¨³æ€çš„<span>$V^*$</span>ä¹‹åï¼Œå³å¯æ ¹æ®ä¸‹å¼å¾—åˆ°å¯¹åº”çš„æœ€ä¼˜Policyï¼š</p><p>$</p><p>\begin{equation} \pi(s) \leftarrow \underset {a \in A} {arg \ max} \left( R(s, a) + \gamma \sum_{s&#39; \in S}P(s&#39; | s,a) V^*(s&#39;) \right) \end{equation} $</p><h3 id="Policy-Iteration"><a class="docs-heading-anchor" href="#Policy-Iteration">Policy Iteration</a><a id="Policy-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-Iteration" title="Permalink"></a></h3><p>Policy Iterationçš„æ€æƒ³æ˜¯ï¼Œå…ˆå°†æ‰€æœ‰çŠ¶æ€å¯¹åº”çš„<span>$V$</span>ç½®ä¸º0ï¼ˆä¹Ÿå¯ä»¥æ ¹æ®å…ˆéªŒè®¾ç½®ç›¸åº”çš„å€¼ï¼‰ï¼Œé’ˆå¯¹æ¯ä¸ªçŠ¶æ€ï¼Œå…ˆéšæœºåˆå§‹åŒ–ä¸€ä¸ªaction(æˆ–æ ¹æ®å‰é¢è®¾ç½®çš„å…ˆéªŒè°ƒæ•´),è®°ä¸º<span>$\pi_0$</span>ï¼Œåœ¨ç»™å®šPolicyä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç®—å‡ºä¸‹ä¸€æ­¥æ‰€æœ‰çŠ¶æ€å¯¹åº”çš„<span>$V(s&#39;)$</span>ï¼Œå³(Policy Evaluation)ï¼š</p><p>$</p><p>\begin{equation} V^{\pi}<em>t (s) = R(s, \pi(s)) + \gamma \sum</em>{s&#39; \in S} P(s&#39;|s, \pi(s)) V^{\pi}_{t+1}(s&#39;) \end{equation} $</p><p>ç„¶åæ ¹æ®æ–°å¾—åˆ°çš„<span>$V_{t+1}(s)$</span>,é‡æ–°è°ƒæ•´Policyï¼Œä½¿å¾—æ¯ä¸ªstateå¯¹åº”çš„actionéƒ½æ˜¯æœç€<span>$V(s&#39;|s)$</span>æœ€å¤§çš„æ–¹å‘åœ¨ç§»åŠ¨ï¼Œå³ï¼ˆPolicy updateï¼‰:</p><p>$</p><p>\begin{equation} \pi<em>{k+1}(s) = \underset {a \in A} {arg \ max} \left( R(s,a) + \gamma \sum</em>{s&#39; \in S} P(s&#39; | s, a) V^{\pi_k}(s&#39;) \right) \end{equation} $</p><p>å¦‚æ­¤è¿­ä»£ï¼Œç›´è‡³Policyä¸å†å˜åŒ–ã€‚å¦‚ä½•è¯æ˜æœ€ä¼˜å‘¢ï¼Ÿï¼ˆTODO: å•è°ƒæ€§ï¼‰</p><p><img src="img/eve_policy_iteration.gif" alt="Policy Iteration"/></p><h2 id="1.3-Day-3:-Monte-Carlo-Methods-in-Depth"><a class="docs-heading-anchor" href="#1.3-Day-3:-Monte-Carlo-Methods-in-Depth">1.3 Day 3: Monte Carlo Methods in Depth</a><a id="1.3-Day-3:-Monte-Carlo-Methods-in-Depth-1"></a><a class="docs-heading-anchor-permalink" href="#1.3-Day-3:-Monte-Carlo-Methods-in-Depth" title="Permalink"></a></h2><p>åœ¨å‰é¢1.2ä¸­ï¼Œæˆ‘ä»¬å°è¯•å°†Eveçš„èµ·ç‚¹æ”¾åœ¨ä¸åŒä½ç½®ï¼Œç„¶åè®¡ç®—æ¯æ¬¡å®éªŒç»“æŸæ—¶rewardçš„æœŸæœ›çš„ä¼°è®¡å€¼ï¼Œä»è€Œç”¨æ¥è¯„ä»·ä¸åŒçŠ¶æ€ä½œä¸ºèµ·å§‹ç‚¹çš„é‡è¦æ€§ã€‚ä¸è¿‡ï¼Œåœ¨å¤§å¤šæ•°é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨çš„ä¸ä»…ä»…æ˜¯æŸä¸€çŠ¶æ€ä½œä¸ºåˆå§‹çŠ¶æ€çš„æƒ…å†µã€‚</p><p><strong>First Visit Monte Carlo Method</strong> æ–¹æ³•çš„æ€æƒ³å°±æ˜¯ï¼ŒAgentåœ¨åšå‡ºä¸€æ¬¡å°è¯•ä¹‹åï¼Œå¾—åˆ°çŠ¶æ€çš„åºåˆ—<span>$s_0, s_1, ... s_T$</span>ï¼ˆå°†å…¶æƒ³è±¡æˆæœç´¢ç©ºé—´ä¸­çš„ä¸€æ¡è·¯å¾„ï¼‰ï¼Œä»¥åŠæœ€ç»ˆçš„<span>$r$</span>ï¼Œç„¶åï¼Œå°†è¯¥rewardåˆ†é…ç»™è·¯å¾„ä¸Šçš„ç¬¬ä¸€æ¬¡é‡åˆ°çš„çŠ¶æ€ï¼Œé‡å¤å¤šæ¬¡ä¹‹åï¼Œè®¡ç®—æ¯ä¸ªçŠ¶æ€ä¸Šrçš„å¹³å‡å€¼ä½œä¸ºè¯¥çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ï¼ˆæ˜¾ç„¶ï¼Œä¹Ÿä¸ä¸€å®šé™åˆ¶åœ¨first visitçš„stateï¼Œåªæ˜¯è¿™æ ·å¤„ç†æ›´è‡ªç„¶äº›ï¼‰ã€‚å‡è®¾æˆ‘ä»¬å¯¹stateä¹‹é—´çš„è½¬æ¢å…³ç³»å¾ˆæ¸…æ¥šï¼ˆå³å·²çŸ¥<strong>model</strong>ï¼‰ï¼Œé‚£ä¹ˆå‰©ä¸‹çš„ä»»åŠ¡å°±æ˜¯æ¯æ¬¡å¾€å‰çœ‹ä¸€æ­¥ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„stateï¼Œè·³è½¬è¿‡å»å³å¯ã€‚å¯¹äºmodelæœªçŸ¥çš„æƒ…å†µï¼Œåˆ™éœ€è¦è€ƒè™‘(state, action) pairï¼Œç„¶åæ ¹æ®å‰é¢first visitçš„æ–¹æ³•æ›´æ–°å¯¹åº”pairçš„ä»·å€¼ï¼Œæœ€åï¼Œåœ¨æ¯ä¸ªçŠ¶æ€ä¸‹æ‰¾åˆ°ä½¿å¾—pairçš„valueæœ€å¤§çš„actionå³å¯ã€‚</p><p>ç±»æ¯”å‰é¢çš„Policy Iterationè¿‡ç¨‹ï¼Œè¿™é‡Œæˆ‘ä»¬åŒæ ·å¯ä»¥å€Ÿç”¨MCå¾—åˆ°çš„q valueï¼Œç„¶åè¿­ä»£ä¼˜åŒ–ã€‚ï¼ˆå…¶å®å°±æ˜¯Generalized Policy Iterationçš„ä¸€ç§ï¼‰è¿™é‡Œæœ‰ä¸¤ä¸ªå‰æå‡è®¾ï¼š</p><ol><li>exploring starts ï¼ˆå¼•å…¥ <span>$\epsilon$</span>-softè§£å†³è¯¥é—®é¢˜ï¼‰</li><li>infinite number of episodes ï¼ˆæ ¹æ®GPIä¸­çš„æ€æƒ³ï¼Œä¸€æ­¥æ­¥ä¼˜åŒ–ï¼‰</li></ol><h3 id="Off-Policy-MC"><a class="docs-heading-anchor" href="#Off-Policy-MC">Off-Policy MC</a><a id="Off-Policy-MC-1"></a><a class="docs-heading-anchor-permalink" href="#Off-Policy-MC" title="Permalink"></a></h3><p>é¦–å…ˆåŒºåˆ†behavior policy å’Œ target policy(å¯ä»¥æ˜¯æŸç§å›ºå®šçš„è´ªå¿ƒç®—æ³•)ã€‚æ ¹æ®behavior policyå¾—å‡ºä¸€æ¡åºåˆ—ï¼Œç„¶åæŒ‰ç…§é‡è¦æ€§é‡‡æ ·æ›´æ–°valueã€‚ï¼ˆTODO:è¿™é‡Œéœ€è¦å±•å¼€ï¼‰</p><h3 id="MCTS"><a class="docs-heading-anchor" href="#MCTS">MCTS</a><a id="MCTS-1"></a><a class="docs-heading-anchor-permalink" href="#MCTS" title="Permalink"></a></h3><p>TODO: è¿™ä¸ªéœ€è¦è®²ä¸ªä¸“é¢˜</p><h2 id="1.4-Day-4:-Temporal-Difference-Learning"><a class="docs-heading-anchor" href="#1.4-Day-4:-Temporal-Difference-Learning">1.4 Day 4: Temporal-Difference Learning</a><a id="1.4-Day-4:-Temporal-Difference-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#1.4-Day-4:-Temporal-Difference-Learning" title="Permalink"></a></h2><p>åœ¨å‰é¢MCéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç­‰åˆ°ä¸€ä¸ªepisodeç»“æŸåï¼Œå¾—åˆ°æœ€ç»ˆçš„rewardï¼Œç„¶åç”¨å…¶æ›´æ–°state valueï¼ŒTDçš„æ€æƒ³åˆ™æ˜¯ï¼Œåªéœ€èµ°ä¸€æ­¥ï¼Œç„¶ååˆ©ç”¨ä¸¤æ­¥ä¹‹é—´state valueçš„å·®åˆ†ä¿®æ­£ä¸Šä¸€æ­¥çš„state value(learn a guess from a guess, model free)ã€‚</p><p>$</p><p>\begin{equation} V(S<em>t) \leftarrow V(S</em>t) + \alpha (R<em>{t+1} + \gamma V(S</em>{t+1} - V(S_t))) \end{equation} $</p><h3 id="SARSA"><a class="docs-heading-anchor" href="#SARSA">SARSA</a><a id="SARSA-1"></a><a class="docs-heading-anchor-permalink" href="#SARSA" title="Permalink"></a></h3><p>ç›´æ¥å­¦ä¹ action valueã€‚</p><p>$</p><p>\begin{equation} Q(S<em>t, A</em>t) \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma Q(S</em>{t+1}, A<em>{t+1}) - Q(S</em>t, A_t)) \end{equation} $</p><p>å˜ç§ï¼ŒExpected SARSA</p><p>$</p><p>\begin{equation} \begin{split} Q(S<em>t, A</em>t)  &amp; \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \mathbb{E} (Q(S</em>{t+1}, A<em>{t+1} | S</em>{t+1}) - Q(S<em>t, A</em>t)) \
&amp; \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \sum</em>{a} \pi(a|S<em>{t+1}) Q(S</em>{t+1}, a) - Q(S<em>t, A</em>t))  \end{split} \end{equation} $</p><p>Q-Learning</p><p>$</p><p>\begin{equation} Q(S<em>t, A</em>t) \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \ \underset {a } {max} \ Q(S</em>{t+1}, a) - Q(S<em>t, A</em>t)) \end{equation} $</p><p>&lt;div class=&quot;alert alert-info&quot;&gt; å…³äºSARSAå’ŒQ-Learningçš„å¯¹æ¯”ï¼Œå¯ä»¥å‚è€ƒè¿™é‡Œçš„&lt;a href=&quot;https://www.google.co.jp/search?q=sarsa+and+q+learning&amp;oq=sarsa+and+q&amp;aqs=chrome.0.0j69i57j0l4.3086j0j1&amp;sourceid=chrome&amp;ie=UTF-8&quot;&gt;Youtube&lt;/a&gt;é“¾æ¥ã€‚ &lt;/div&gt;</p><p>è¿›ä¸€æ­¥ï¼Œ[Double Q-Learning][]ï¼Œç®—æ˜¯ä¸€ç§off-policyï¼Œbehaviorå’Œtargetéšæœºç›¸äº’å­¦ä¹ ã€‚æ‘˜è¦éƒ¨åˆ†è¯´å¾—å¾ˆæ¸…æ¥šäº†ï¼š</p><blockquote><p>In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. </p></blockquote><h1 id="2.-Approximate-Methods"><a class="docs-heading-anchor" href="#2.-Approximate-Methods">2. Approximate Methods</a><a id="2.-Approximate-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Approximate-Methods" title="Permalink"></a></h1><blockquote><p>â€œè¡£å¸¦æ¸å®½ç»ˆä¸æ‚”ï¼Œä¸ºä¼Šæ¶ˆå¾—äººæ†”æ‚´ã€‚â€</p></blockquote><h1 id="3.-Deep-Reinforcement-Learning"><a class="docs-heading-anchor" href="#3.-Deep-Reinforcement-Learning">3. Deep Reinforcement Learning</a><a id="3.-Deep-Reinforcement-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Deep-Reinforcement-Learning" title="Permalink"></a></h1><blockquote><p>â€œä¼—é‡Œå¯»ä»–åƒç™¾åº¦ï¼Œè“¦ç„¶å›é¦–ï¼Œé‚£äººå´åœ¨ç¯ç«é˜‘çŠå¤„ã€‚â€</p></blockquote><h1 id="å‚è€ƒæ–‡çŒ®"><a class="docs-heading-anchor" href="#å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</a><a id="å‚è€ƒæ–‡çŒ®-1"></a><a class="docs-heading-anchor-permalink" href="#å‚è€ƒæ–‡çŒ®" title="Permalink"></a></h1><h2 id="Books"><a class="docs-heading-anchor" href="#Books">Books</a><a id="Books-1"></a><a class="docs-heading-anchor-permalink" href="#Books" title="Permalink"></a></h2><ol><li>[Reinforcement Learning: An Introduction. Second edition][]</li><li>[Decision Making Under Uncertainty Theory and Application][]</li><li>[Artificial Intelligence: Foundations of Computational Agents, 2nd Edition][]</li><li>[Approximate Dynamic Programming: Solving the Curses of Dimensionality, 2nd Edition][]</li><li>[Dynamic Programming and Optimal Control][]</li><li>[Reinforcement Learning State of the Art][]</li><li>[reinforcement learning and dynamic programming using function approximators][]</li></ol><p>[Reinforcement Learning: An Introduction. Second edition]: http://incompleteideas.net/book/the-book-2nd.html [Decision Making Under Uncertainty Theory and Application]: https://mitpress.mit.edu/decision-making-under-uncertainty [Artificial Intelligence: Foundations of Computational Agents, 2nd Edition]: http://artint.info/ [Approximate Dynamic Programming: Solving the Curses of Dimensionality, 2nd Edition]: http://adp.princeton.edu/ [Dynamic Programming and Optimal Control]: http://www.athenasc.com/dpbook.html [Reinforcement Learning State of the Art]: https://link.springer.com/book/10.1007%2F978-3-642-27645-3 [reinforcement learning and dynamic programming using function approximators]: http://rlbook.busoniu.net/</p><h2 id="Papers"><a class="docs-heading-anchor" href="#Papers">Papers</a><a id="Papers-1"></a><a class="docs-heading-anchor-permalink" href="#Papers" title="Permalink"></a></h2><ol><li>[Double Q-Learning][]</li></ol><p>[Double Q-Learning]: https://papers.nips.cc/paper/3964-double-q-learning.pdf</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
