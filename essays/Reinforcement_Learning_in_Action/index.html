<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>强化学习实战[updating] · Jun Tian</title><meta name="title" content="强化学习实战[updating] · Jun Tian"/><meta property="og:title" content="强化学习实战[updating] · Jun Tian"/><meta property="twitter:title" content="强化学习实战[updating] · Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">👋 About</a></li><li><span class="tocitem">💻 Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">📖 Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">🙋 Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">🔗 Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>强化学习实战[updating]</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>强化学习实战[updating]</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/Reinforcement_Learning_in_Action/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: Algorithm,Julia,ReinforcementLearning CJKmainfont: KaiTi –-</p><h1 id="强化学习实战[updating]"><a class="docs-heading-anchor" href="#强化学习实战[updating]">强化学习实战[updating]</a><a id="强化学习实战[updating]-1"></a><a class="docs-heading-anchor-permalink" href="#强化学习实战[updating]" title="Permalink"></a></h1><p>[等我用Julia写个RL的库了再来继续更新。]</p><p>我一直希望找到一篇长文，能从实用的角度讲清楚强化学习。一年过去了，我决定自己动手写这样一篇文章。</p><h2 id="编程语言"><a class="docs-heading-anchor" href="#编程语言">编程语言</a><a id="编程语言-1"></a><a class="docs-heading-anchor-permalink" href="#编程语言" title="Permalink"></a></h2><p>本文将使用Julia作为主要的编程语言来实现Reinforcement Learning中的大多数算法。</p><p>Why Julia?事实上，我更乐意使用Clojure，然而其基础工具库实在太匮乏，不得不放弃。Why not Python?我只是单纯觉得Julia写出来的代码更好看！不过阅读本文并不需要读者熟练掌握Julia，只需要有基本的编程思想即可，我会尽量将Julia这门语言独有的内容降到最低（必要的时候会给出解释和参考文献）。</p><h1 id="1.-Tabular-Methods"><a class="docs-heading-anchor" href="#1.-Tabular-Methods">1. Tabular Methods</a><a id="1.-Tabular-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Tabular-Methods" title="Permalink"></a></h1><blockquote><p>“昨夜西风凋碧树，独上高楼，望尽天涯路。”</p></blockquote><h2 id="1.1-Day-1:-Where&#39;s-Eve?"><a class="docs-heading-anchor" href="#1.1-Day-1:-Where&#39;s-Eve?">1.1 Day 1: Where&#39;s Eve?</a><a id="1.1-Day-1:-Where&#39;s-Eve?-1"></a><a class="docs-heading-anchor-permalink" href="#1.1-Day-1:-Where&#39;s-Eve?" title="Permalink"></a></h2><p><img src="img/eve_move.png" alt="eve_move"/></p><p>假设某个虚拟的世界中只有两个星球（黄色星球[Y]和红色星球[R]），每个星球上各有一枚硬币（正面朝上的概率本别是<span>$p$</span>, <span>$q$</span>），我们的机器人朋友Eve的初始位置在左边的黄色星球上(<span>$s_0=Y$</span>)，接下来，它每天都尝试投掷当前所在星球上的硬币，如果正面朝上，那么它移动到另外一个星球上，否则呆在原地不动。于是，我们可以计算出第<span>$t$</span>天Eve处于黄色星球的概率:<span>$P(s_t=Y)=P(s_{t-1}=Y)(1-p) + P(s_{t-1}=R)(q)$</span>，类似的，处于红色星球的概率为：<span>$P(s_t=R) = P(s_{t-1}=Y)(p) + P(X_{t-1}=R)(1-q)$</span>.将其写成概率转移矩阵的形式如下：</p><p>$</p><p>\begin{equation} \boldsymbol{P} = \left( \begin{matrix} 1-p &amp; p \
q &amp; 1-q \end{matrix} \right) \end{equation} $</p><p>于是，<span>$t$</span>时刻Eve所处状态的概率为：</p><p>$</p><p>\begin{equation} \boldsymbol{s}<em>t = \boldsymbol{s}</em>0 \boldsymbol{P}^t \end{equation} $ 这里，<span>$\boldsymbol{s}_0=(1,0)$</span>，即初始状态处于左边的黄色星球。由于该概率转移矩阵的特殊性质，<span>$\boldsymbol{s}_t$</span>最终会收敛到一个稳态。下图为<span>$(p,q)$</span>分别取(0.5,0.5),(0.2, 0.1),(0.95,0.7)时，<span>$\boldsymbol{s}_t$</span>的收敛过程：</p><p><img src="img/eve_planet.png" alt="eve_planet"/></p><p>假设我们的机器人Eve落在了一条数轴的原点处，它的任务是寻找到生命，在-3和5处分别有一盆花和一只小狗，Eve每天只能选择向左或向右移动一个单位，只要发现生命（花或者狗）后，任务便结束，然后Eve返回并获得相应的奖励（假设奖励分别是3和5，其它位置没有奖励）。 显然，对于这样一个简单而且确定的问题而言，有多种搜索的方法得到解，这里先假设Eve并不知道环境是确定的。</p><p><img src="img/Eve_Example.png" alt="Eve_Example"/></p><p>现在先给出如下定义：</p><ul><li><p class="math-container">\[\mathcal{A}\]</p>：Actions, <strong>动作空间</strong>,在这里包含两种可能<span>${:left, :right}$</span></li><li><p class="math-container">\[a_t\]</p>: 在<span>$t$</span>时刻采取的行动</li><li><p class="math-container">\[\mathcal{S}\]</p>: States, <strong>状态空间</strong>，这里就是Eve所有可能的位置<span>${-3, -2, -1, 0, 1, 2, 3, 4, 5}$</span></li><li><p class="math-container">\[s_t\]</p>: 在<span>$t$</span>时刻所处的状态</li><li><p class="math-container">\[\mathcal{R}\]</p>:所有可能的奖励，这里由三个离散值<span>${0, 3, 5}$</span>构成</li><li><p class="math-container">\[R_t\]</p>：每一天获得的奖励，特别地，我们将游戏结束时的时间记为<span>$T$</span>，任务结束时获得的奖励为<span>$R_T$</span></li></ul><p>假设Eve很笨，它的记忆只有一天（并不记得它曾走过哪些地方），每天只会等概率地随机选择向左或向右移动，将其记作<strong>策略</strong><span>$\pi$</span>（尽管目前还只是随机游走，谈不上什么策略）。我们先观察以下两个指标：</p><ol><li><p class="math-container">\[\bar{R_T} = \frac{\sum_{i=1}^{N} R_T^i}{N}\]</p>，N次试验的平均收益(<span>$3.746$</span>)</li></ol><p><img src="img/eve_reward_count.png" alt="任务结束时获得的奖励分布"/></p><ol><li><p class="math-container">\[\bar{T} = \frac{\sum_{i=1}^{N} T^i}{N}\]</p>，N次试验平均行动的次数(<span>$16.164$</span>)</li></ol><p><img src="img/eve_steps_count.png" alt="任务结束时，行动次数的分布"/></p><p>平均收益只有3.7左右，离最优解（5）还有点距离，此外平均实验次数居然到了16次。接下来，我们将一步步放松约束条件，改进Eve的行动策略。</p><p>这里先将前面Eve与环境交互的过程用下图抽象出来：</p><p><img src="img/MDP_general_representation.png" alt="MDP的序列化描述"/></p><p>每一步的reward只由当前时刻的state和action共同决定（这里对于Eve来说，每天的reward其实只与state相关，环境并不受Eve的行动影响，当然，真实情况要比这复杂得多，我们一步步来），而下一时刻的state则只受上一个时刻的state和action共同决定。</p><h2 id="1.2-Day-2:-The-Value-of-State"><a class="docs-heading-anchor" href="#1.2-Day-2:-The-Value-of-State">1.2 Day 2: The Value of State</a><a id="1.2-Day-2:-The-Value-of-State-1"></a><a class="docs-heading-anchor-permalink" href="#1.2-Day-2:-The-Value-of-State" title="Permalink"></a></h2><p>某一天，Eve感到很迷茫，它不知道未来这么走下去，收益究竟有多少，我们可以给Eve算一下未来收益的总和<span>$G$</span>：</p><p>$</p><p>\begin{equation} G<em>t = R</em>t + R<em>{t+1} + R</em>{t+2} ... \end{equation} $</p><p>不过，Eve可能并不这么认为，同样的收益，第5天得到还是第3天得到对于Eve来说有着不同的意义（显然后者的意义更大），所以不妨给每天的收益增加一个基于时间的折扣系数<span>$\gamma$</span>（这么做有许多好处，数学上处理起来更方便，而且对于infinite的问题也更容易求解，当然，根据实际问题不同，你完全可以设计不同的<span>$G_t$</span>计算方式，比如averaged reward）。</p><p>$</p><p>\begin{equation} G<em>t = R</em>t + \gamma R<em>{t+1} + \gamma^2 R</em>{t+2} + ... \end{equation} $</p><p>那么，根据Eve在<span>$t$</span>时刻所处的状态不同，假设可以算出收益的期望，称作value function:</p><p>$</p><p>\begin{equation} V<em>t(s) = \mathbb{E}(G</em>t | s) = \mathbb{E}(R<em>t + \gamma R</em>{t+1} + \gamma^2 R<em>{t+2} + ... | s) \label{value</em>equation} \end{equation} $ 从上帝视角来看，在环境不发生变化的情况下，显然每个状态s都有一个对应的固定的<span>$V(s)$</span>，这样，每次Eve决定怎么走的时候，只需要看下这张表，从可以到达的所有状态中，找到价值最高的状态<span>$s&#39;$</span>，跳转过去即可。现在问题变成了如何计算<span>$V(s)$</span>。我们不妨先模拟下，将Eve放在不同的位置，然后分别执行原来的随机策略，计算平均收益<span>$\hat{G_t}$</span>，来估计<span>$V(s)$</span>。</p><p><img src="img/eve_v_estimate.png" alt="随机模拟估计V(s)，gamma=1.0"/></p><p>看起来上图似乎符合我们的直观感受。<span>$\gamma$</span>越大，我们就越是看重长期收益，反之则更看重短期收益。</p><p>不过，对我们的机器人Eve来说，手上并没有这样一张表，前面虽然通过模拟得到了这样一张表，但是效率还很低。那是否能找到解析解呢？首先引入状态转移的概念，这里Eve每一时刻所处的状态只与其上一时候所处的状态（和行动）有关，不受更早时候状态（和行为）的影响（即具有Markov性质），于是有：</p><p>$</p><p>\begin{equation} P(s&#39;|s) = \sum_{a \in A} P(s&#39; | s, a) \end{equation} $</p><p>对于前面Eve机器人随机策略而言，对应的概率转移矩阵为：</p><p>$</p><p>\begin{equation} \begin{bmatrix} 1.0  &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0         \
0.5 &amp;0 &amp; 0.5  &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0      \
0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0      \
0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0 &amp;0     \
0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0 &amp;0 &amp;0    \
0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5  &amp;0 &amp;0  \
0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 &amp;0  \
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0.5 &amp;0 &amp; 0.5 \
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0   &amp;0 &amp; 1.0 \
\end{bmatrix} \end{equation} $</p><p>于是，<span>$\eqref{value_equation}$</span>可以写成：</p><p>$</p><p>\begin{equation} V<em>t(s) = R</em>t(s) + \gamma \sum<em>{s&#39; \in S} P(s&#39; | s) V</em>{t+1}(s&#39;) \end{equation} $</p><p>上式构建了不同时刻的价值函数之间的关系，在这里由于状态转移函数是固定的，最终会达到一个稳态（TODO: 这里需要展开说下，MRP），即：</p><p>$</p><p>\begin{equation} \begin{bmatrix} V^<em>(s_1) \
\dots \
V^</em>(s<em>N)  \end{bmatrix} =  \begin{bmatrix} R(s</em>1) \
\dots \
R(s<em>N)  \end{bmatrix} + \gamma \begin{bmatrix} P(s</em>1|s<em>1) &amp; \dots &amp;P(s</em>N|s<em>1)\
\vdots  &amp;\ddots &amp;\vdots \
P(s</em>1|s<em>N) &amp;\dots &amp;P(s</em>N|s<em>N)  \end{bmatrix} \begin{bmatrix} V^*(s</em>1) \
\dots \
V^*(s_N)  \end{bmatrix} \end{equation} $</p><p>采用矩阵的表示方法即是：</p><p>$</p><p>\begin{equation} \begin{split} \boldsymbol{V} &amp;= \boldsymbol{R} + \gamma \boldsymbol{P V}\
\boldsymbol{V} &amp;= (\boldsymbol{I} - \gamma \boldsymbol{P})^{-1} \boldsymbol{R} \end{split} \end{equation} $</p><pre><code class="language-julia hljs">P = diagm(repmat([0.5], 8), 1) + diagm(repmat([0.5], 8), -1)
P[1,1], P[1,2], P[end, end-1], P[end, end] = 1, 0, 0, 1
V(γ) = (diagm(ones(9)) -  γ*P)^-1 * R
show(V(0.9))
# [30.0, 19.9416, 14.3146, 11.8686, 12.0601, 14.9316, 21.1213, 32.0046, 50.0]
show(V(0.95))
# [60.0, 48.1992, 41.4721, 39.1104, 40.8656, 46.9225, 57.9186, 75.0113, 100.0]
show(V(0.99))
# [300.0, 301.464, 309.018, 322.814, 343.132, 370.383, 405.115, 448.032, 500.0]</code></pre><p>稠密矩阵求逆的复杂度为<span>$O(n^3)$</span>（TODO: 这里需要插入相关文献），接下来，我们将分别采用Value Iteration和 Policy Iteration来求解。</p><h3 id="Value-Iteration"><a class="docs-heading-anchor" href="#Value-Iteration">Value Iteration</a><a id="Value-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Value-Iteration" title="Permalink"></a></h3><p>让我们先从上帝视角来看看这个问题。我们的本意是希望得到一张表，让Eve每次行动的时候，能够查看它从当前位置往左和往右走的期望价值，然后从中选出期望价值最大的行动<span>$a$</span>作为决策Policy的结果。显然，最糟糕的情况下，我们需要遍历所有的可能<span>$|A|^{|S|}$</span>，不过通过Value Iteration，我们可以先找到稳态的<span>$V(s)$</span>，然后直接得出最优的Policy。可以证明在最优策略下（TODO: Reference Needed Here, Bellman Function），价值函数满足以下式子：</p><p>$</p><p>\begin{equation} V^<em>(s) = \underset{a \in A}{max} \left( R(s, a) + \gamma \sum_{s&#39; \in S}P(s&#39; | s,a) V^</em>(s&#39;) \right) \end{equation} $</p><p>于是，我们可以先随机初始化<span>$V(s)$</span>，然后根据上式迭代计算并更新，直至收敛。下图动态地展示了该过程：</p><p><img src="img/eve_v_iteration_gamma_0.9.gif" alt="gamma=0.9时，Value 迭代的过程"/></p><p>不同gamma对应的稳态Value:</p><p><img src="img/eve_v_iteration.png" alt="不同gamma对应的稳态Value"/></p><p>在得到稳态的<span>$V^*$</span>之后，即可根据下式得到对应的最优Policy：</p><p>$</p><p>\begin{equation} \pi(s) \leftarrow \underset {a \in A} {arg \ max} \left( R(s, a) + \gamma \sum_{s&#39; \in S}P(s&#39; | s,a) V^*(s&#39;) \right) \end{equation} $</p><h3 id="Policy-Iteration"><a class="docs-heading-anchor" href="#Policy-Iteration">Policy Iteration</a><a id="Policy-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-Iteration" title="Permalink"></a></h3><p>Policy Iteration的思想是，先将所有状态对应的<span>$V$</span>置为0（也可以根据先验设置相应的值），针对每个状态，先随机初始化一个action(或根据前面设置的先验调整),记为<span>$\pi_0$</span>，在给定Policy之后，我们可以算出下一步所有状态对应的<span>$V(s&#39;)$</span>，即(Policy Evaluation)：</p><p>$</p><p>\begin{equation} V^{\pi}<em>t (s) = R(s, \pi(s)) + \gamma \sum</em>{s&#39; \in S} P(s&#39;|s, \pi(s)) V^{\pi}_{t+1}(s&#39;) \end{equation} $</p><p>然后根据新得到的<span>$V_{t+1}(s)$</span>,重新调整Policy，使得每个state对应的action都是朝着<span>$V(s&#39;|s)$</span>最大的方向在移动，即（Policy update）:</p><p>$</p><p>\begin{equation} \pi<em>{k+1}(s) = \underset {a \in A} {arg \ max} \left( R(s,a) + \gamma \sum</em>{s&#39; \in S} P(s&#39; | s, a) V^{\pi_k}(s&#39;) \right) \end{equation} $</p><p>如此迭代，直至Policy不再变化。如何证明最优呢？（TODO: 单调性）</p><p><img src="img/eve_policy_iteration.gif" alt="Policy Iteration"/></p><h2 id="1.3-Day-3:-Monte-Carlo-Methods-in-Depth"><a class="docs-heading-anchor" href="#1.3-Day-3:-Monte-Carlo-Methods-in-Depth">1.3 Day 3: Monte Carlo Methods in Depth</a><a id="1.3-Day-3:-Monte-Carlo-Methods-in-Depth-1"></a><a class="docs-heading-anchor-permalink" href="#1.3-Day-3:-Monte-Carlo-Methods-in-Depth" title="Permalink"></a></h2><p>在前面1.2中，我们尝试将Eve的起点放在不同位置，然后计算每次实验结束时reward的期望的估计值，从而用来评价不同状态作为起始点的重要性。不过，在大多数问题中，我们关注的不仅仅是某一状态作为初始状态的情况。</p><p><strong>First Visit Monte Carlo Method</strong> 方法的思想就是，Agent在做出一次尝试之后，得到状态的序列<span>$s_0, s_1, ... s_T$</span>（将其想象成搜索空间中的一条路径），以及最终的<span>$r$</span>，然后，将该reward分配给路径上的第一次遇到的状态，重复多次之后，计算每个状态上r的平均值作为该状态的价值估计（显然，也不一定限制在first visit的state，只是这样处理更自然些）。假设我们对state之间的转换关系很清楚（即已知<strong>model</strong>），那么剩下的任务就是每次往前看一步，找到最优的state，跳转过去即可。对于model未知的情况，则需要考虑(state, action) pair，然后根据前面first visit的方法更新对应pair的价值，最后，在每个状态下找到使得pair的value最大的action即可。</p><p>类比前面的Policy Iteration过程，这里我们同样可以借用MC得到的q value，然后迭代优化。（其实就是Generalized Policy Iteration的一种）这里有两个前提假设：</p><ol><li>exploring starts （引入 <span>$\epsilon$</span>-soft解决该问题）</li><li>infinite number of episodes （根据GPI中的思想，一步步优化）</li></ol><h3 id="Off-Policy-MC"><a class="docs-heading-anchor" href="#Off-Policy-MC">Off-Policy MC</a><a id="Off-Policy-MC-1"></a><a class="docs-heading-anchor-permalink" href="#Off-Policy-MC" title="Permalink"></a></h3><p>首先区分behavior policy 和 target policy(可以是某种固定的贪心算法)。根据behavior policy得出一条序列，然后按照重要性采样更新value。（TODO:这里需要展开）</p><h3 id="MCTS"><a class="docs-heading-anchor" href="#MCTS">MCTS</a><a id="MCTS-1"></a><a class="docs-heading-anchor-permalink" href="#MCTS" title="Permalink"></a></h3><p>TODO: 这个需要讲个专题</p><h2 id="1.4-Day-4:-Temporal-Difference-Learning"><a class="docs-heading-anchor" href="#1.4-Day-4:-Temporal-Difference-Learning">1.4 Day 4: Temporal-Difference Learning</a><a id="1.4-Day-4:-Temporal-Difference-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#1.4-Day-4:-Temporal-Difference-Learning" title="Permalink"></a></h2><p>在前面MC部分中，我们需要等到一个episode结束后，得到最终的reward，然后用其更新state value，TD的思想则是，只需走一步，然后利用两步之间state value的差分修正上一步的state value(learn a guess from a guess, model free)。</p><p>$</p><p>\begin{equation} V(S<em>t) \leftarrow V(S</em>t) + \alpha (R<em>{t+1} + \gamma V(S</em>{t+1} - V(S_t))) \end{equation} $</p><h3 id="SARSA"><a class="docs-heading-anchor" href="#SARSA">SARSA</a><a id="SARSA-1"></a><a class="docs-heading-anchor-permalink" href="#SARSA" title="Permalink"></a></h3><p>直接学习action value。</p><p>$</p><p>\begin{equation} Q(S<em>t, A</em>t) \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma Q(S</em>{t+1}, A<em>{t+1}) - Q(S</em>t, A_t)) \end{equation} $</p><p>变种，Expected SARSA</p><p>$</p><p>\begin{equation} \begin{split} Q(S<em>t, A</em>t)  &amp; \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \mathbb{E} (Q(S</em>{t+1}, A<em>{t+1} | S</em>{t+1}) - Q(S<em>t, A</em>t)) \
&amp; \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \sum</em>{a} \pi(a|S<em>{t+1}) Q(S</em>{t+1}, a) - Q(S<em>t, A</em>t))  \end{split} \end{equation} $</p><p>Q-Learning</p><p>$</p><p>\begin{equation} Q(S<em>t, A</em>t) \leftarrow Q(S<em>t, A</em>t) + \alpha (R<em>{t+1} + \gamma \ \underset {a } {max} \ Q(S</em>{t+1}, a) - Q(S<em>t, A</em>t)) \end{equation} $</p><p>&lt;div class=&quot;alert alert-info&quot;&gt; 关于SARSA和Q-Learning的对比，可以参考这里的&lt;a href=&quot;https://www.google.co.jp/search?q=sarsa+and+q+learning&amp;oq=sarsa+and+q&amp;aqs=chrome.0.0j69i57j0l4.3086j0j1&amp;sourceid=chrome&amp;ie=UTF-8&quot;&gt;Youtube&lt;/a&gt;链接。 &lt;/div&gt;</p><p>进一步，[Double Q-Learning][]，算是一种off-policy，behavior和target随机相互学习。摘要部分说得很清楚了：</p><blockquote><p>In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. </p></blockquote><h1 id="2.-Approximate-Methods"><a class="docs-heading-anchor" href="#2.-Approximate-Methods">2. Approximate Methods</a><a id="2.-Approximate-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Approximate-Methods" title="Permalink"></a></h1><blockquote><p>“衣带渐宽终不悔，为伊消得人憔悴。”</p></blockquote><h1 id="3.-Deep-Reinforcement-Learning"><a class="docs-heading-anchor" href="#3.-Deep-Reinforcement-Learning">3. Deep Reinforcement Learning</a><a id="3.-Deep-Reinforcement-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Deep-Reinforcement-Learning" title="Permalink"></a></h1><blockquote><p>“众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”</p></blockquote><h1 id="参考文献"><a class="docs-heading-anchor" href="#参考文献">参考文献</a><a id="参考文献-1"></a><a class="docs-heading-anchor-permalink" href="#参考文献" title="Permalink"></a></h1><h2 id="Books"><a class="docs-heading-anchor" href="#Books">Books</a><a id="Books-1"></a><a class="docs-heading-anchor-permalink" href="#Books" title="Permalink"></a></h2><ol><li>[Reinforcement Learning: An Introduction. Second edition][]</li><li>[Decision Making Under Uncertainty Theory and Application][]</li><li>[Artificial Intelligence: Foundations of Computational Agents, 2nd Edition][]</li><li>[Approximate Dynamic Programming: Solving the Curses of Dimensionality, 2nd Edition][]</li><li>[Dynamic Programming and Optimal Control][]</li><li>[Reinforcement Learning State of the Art][]</li><li>[reinforcement learning and dynamic programming using function approximators][]</li></ol><p>[Reinforcement Learning: An Introduction. Second edition]: http://incompleteideas.net/book/the-book-2nd.html [Decision Making Under Uncertainty Theory and Application]: https://mitpress.mit.edu/decision-making-under-uncertainty [Artificial Intelligence: Foundations of Computational Agents, 2nd Edition]: http://artint.info/ [Approximate Dynamic Programming: Solving the Curses of Dimensionality, 2nd Edition]: http://adp.princeton.edu/ [Dynamic Programming and Optimal Control]: http://www.athenasc.com/dpbook.html [Reinforcement Learning State of the Art]: https://link.springer.com/book/10.1007%2F978-3-642-27645-3 [reinforcement learning and dynamic programming using function approximators]: http://rlbook.busoniu.net/</p><h2 id="Papers"><a class="docs-heading-anchor" href="#Papers">Papers</a><a id="Papers-1"></a><a class="docs-heading-anchor-permalink" href="#Papers" title="Permalink"></a></h2><ol><li>[Double Q-Learning][]</li></ol><p>[Double Q-Learning]: https://papers.nips.cc/paper/3964-double-q-learning.pdf</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
