<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Relation Extraction ¬∑ Jun Tian</title><meta name="title" content="Relation Extraction ¬∑ Jun Tian"/><meta property="og:title" content="Relation Extraction ¬∑ Jun Tian"/><meta property="twitter:title" content="Relation Extraction ¬∑ Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">üëã About</a></li><li><span class="tocitem">üíª Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">üìñ Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">üôã Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">üîó Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Relation Extraction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Relation Extraction</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/RelationExtraction/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: Survey,RelationExtraction CJKmainfont: KaiTi ‚Äì-</p><h1 id="Relation-Extraction"><a class="docs-heading-anchor" href="#Relation-Extraction">Relation Extraction</a><a id="Relation-Extraction-1"></a><a class="docs-heading-anchor-permalink" href="#Relation-Extraction" title="Permalink"></a></h1><p>By <strong>Jun Tian</strong></p><p>Slides/Papers/Data are removed. (2021-08-12)</p><p>2018-03-21</p><p>The task of relation extraction(RE) is to extract links between pairs of nominals.</p><blockquote><p>Given a sentence <span>$S$</span> with annotated pairs of nominals <span>$e_1$</span> and <span>$e_2$</span>, we aim to identify the relations between <span>$e_1$</span> and <span>$e_2$</span>.</p></blockquote><p>Typical relation types include <em>birthdate(PER, DATE)</em>, and <em>founder-of(PER, ORG)</em>, with examples for relations being <em>birthdate(John Smith, 1985-01-01)</em> or <em>founder-of(Bill Gates, Microsoft)</em>.</p><p><img src="Resources/TypicalRelationExtractionPipeline.png" alt="TypicalRelationExtractionPipeline.png"/></p><h2 id="Survey"><a class="docs-heading-anchor" href="#Survey">Survey</a><a id="Survey-1"></a><a class="docs-heading-anchor-permalink" href="#Survey" title="Permalink"></a></h2><h3 id="Papers"><a class="docs-heading-anchor" href="#Papers">Papers</a><a id="Papers-1"></a><a class="docs-heading-anchor-permalink" href="#Papers" title="Permalink"></a></h3><ol><li>‚≠ê‚≠ê‚≠ê[Pawar, Sachin, Girish K. Palshikar, and Pushpak Bhattacharyya. &quot;Relation Extraction: A Survey.&quot; arXiv preprint arXiv:1712.05191 (2017).][]</li><li>‚≠ê‚≠ê‚≠ê[Maynard, Diana, Kalina Bontcheva, and Isabelle Augenstein. &quot;Natural language processing for the semantic web.&quot; Synthesis Lectures on the Semantic Web: Theory and Technology 6.2 (2016): 1-194.][]</li></ol><p>Chapter 4. Relation Extraction.</p><ol><li>‚≠ê‚≠ê[Li, Juanzi, et al. &quot;Knowledge Graph and Semantic Computing.&quot;][]</li></ol><p>P50. A Survey on Relation Extraction</p><ol><li>[Kumar, Shantanu. &quot;A Survey of Deep Learning Methods for Relation Extraction.&quot; arXiv preprint arXiv:1705.03645 (2017).][]</li></ol><p>Mostly are CNN</p><h3 id="Slides"><a class="docs-heading-anchor" href="#Slides">Slides</a><a id="Slides-1"></a><a class="docs-heading-anchor-permalink" href="#Slides" title="Permalink"></a></h3><ol><li>‚≠ê‚≠ê‚≠ê<a href="Paper/2015_EMNLP_Nakov_tutorial_Semantic_Relations.pdf">Learning Semantic Relations from Text 2015 EMNLP</a></li><li><a href="Paper/cse517wi13-RelationExtraction.pdf">Relation Extraction 2013</a></li><li><a href="Paper/A-survey-on-Relation-Extraction-Slides.pdf">A-survey-on-Relation-Extraction-Slides.pdf</a></li></ol><h2 id="Approaches"><a class="docs-heading-anchor" href="#Approaches">Approaches</a><a id="Approaches-1"></a><a class="docs-heading-anchor-permalink" href="#Approaches" title="Permalink"></a></h2><h3 id="Bootstrapping-Methods"><a class="docs-heading-anchor" href="#Bootstrapping-Methods">Bootstrapping Methods</a><a id="Bootstrapping-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping-Methods" title="Permalink"></a></h3><ol><li>‚≠ê[Brin, Sergey. &quot;Extracting patterns and relations from the world wide web.&quot; International Workshop on The World Wide Web and Databases. Springer, Berlin, Heidelberg, 1998.][]</li></ol><p><img src="Resources/DIPRE.png" alt="DIPRE.png"/></p><ol><li>[Etzioni, Oren, et al. &quot;Web-scale information extraction in knowitall:(preliminary results).&quot; Proceedings of the 13th international conference on World Wide Web. ACM, 2004.][]</li><li>‚≠ê[Carlson, Andrew, et al. &quot;Toward an architecture for never-ending language learning.&quot; AAAI. Vol. 5. 2010.][]</li></ol><p><img src="Resources/NELL.png" alt="NELL.png"/></p><ol><li>[Pedro, Saulo DS, and Estevam R. Hruschka. &quot;Conversing learning: Active learning and active social interaction for human supervision in never-ending learning systems.&quot; Ibero-American Conference on Artificial Intelligence. Springer, Berlin, Heidelberg, 2012.][]</li></ol><h3 id="Rule-Based-Approaches"><a class="docs-heading-anchor" href="#Rule-Based-Approaches">Rule Based Approaches</a><a id="Rule-Based-Approaches-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-Based-Approaches" title="Permalink"></a></h3><p>There are two different types of rule-based approaches:</p><ul><li>Those which are stand-alone (<strong>poor recall</strong>, unable to generalize to <strong>unseen patterns</strong>).</li><li>Those which learn rules for inference to complement other relation extraction approaches.</li></ul><ol><li>[Soderland, Stephen G. Learning text analysis rules for domain-specific natural language processing. Diss. University of Massachusetts at Amherst, 1997.][]</li><li>[Reiss, Frederick, et al. &quot;An algebraic approach to rule-based information extraction.&quot; Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on. IEEE, 2008.][]</li><li>‚≠ê[Dong, Xin, et al. &quot;Knowledge vault: A web-scale approach to probabilistic knowledge fusion.&quot; Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.][]</li></ol><p>This starts with a pair of entities that are known to be in a relation according to a (seed) knowledge base, then performs random walk over the knowledge graph to find other paths that connect these entities.</p><h3 id="Supervised-Approaches"><a class="docs-heading-anchor" href="#Supervised-Approaches">Supervised Approaches</a><a id="Supervised-Approaches-1"></a><a class="docs-heading-anchor-permalink" href="#Supervised-Approaches" title="Permalink"></a></h3><p><strong>Pros</strong>: Best performance, provided that a sufficient amount of labeled training data is available. <strong>Cons</strong>: Labeled training data is expensive to produce and thus limited in quantity.</p><h4 id="Traditional-Methods"><a class="docs-heading-anchor" href="#Traditional-Methods">Traditional Methods</a><a id="Traditional-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Traditional-Methods" title="Permalink"></a></h4><p>Typical features:</p><ul><li>n-grams of words to left and right of entities;</li><li>n-grams of POS of words to left and right of entities;</li><li>flag indicating which entity came first in sentence;</li><li>sequence of POS tags and bag of words (BOW) between the two entities;</li><li>dependency path between subject and object;</li><li>POS tags of words on the dependency path between the two entities; and</li><li>lemmas on the dependency path.</li><li>relation embeddings</li></ul><ol><li>[Kambhatla, Nanda. &quot;Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.&quot; Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics, 2004.][]</li></ol><h4 id="Neural-Network-Methods"><a class="docs-heading-anchor" href="#Neural-Network-Methods">Neural Network Methods</a><a id="Neural-Network-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Network-Methods" title="Permalink"></a></h4><ol><li>‚≠ê‚≠ê‚≠ê[Socher, Richard, et al. &quot;Semantic compositionality through recursive matrix-vector spaces.&quot; Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.][]</li><li>[Hashimoto, Kazuma, et al. &quot;Simple customization of recursive neural networks for semantic relation classification.&quot; Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. 2013.][]</li><li>[Zhang, Dongxu, and Dong Wang. &quot;Relation Classification: CNN or RNN?.&quot; Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016. 665-675.][]</li></ol><h5 id="Convolution-Neural-Network"><a class="docs-heading-anchor" href="#Convolution-Neural-Network">Convolution Neural Network</a><a id="Convolution-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution-Neural-Network" title="Permalink"></a></h5><ol><li>‚≠ê‚≠ê‚≠ê[Zeng, Daojian, et al. &quot;Relation classification via convolutional deep neural network.&quot; Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. 2014.][]</li></ol><p>Lexical and sentence level features.   <img src="Resources/cnn_zeng.png" alt="cnn_zeng"/></p><ol><li>[Nguyen, Thien Huu, and Ralph Grishman. &quot;Relation extraction: Perspective from convolutional neural networks.&quot; Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing. 2015.][]</li></ol><p><img src="Resources/perspective_cnn.png" alt="perspective_cnn"/></p><ol><li>[Santos, Cicero Nogueira dos, Bing Xiang, and Bowen Zhou. &quot;Classifying relations by ranking with convolutional neural networks.&quot; arXiv preprint arXiv:1504.06580 (2015).][]</li></ol><p>Ranking is introduced.</p><ol><li>[Xu, Kun, et al. &quot;Semantic relation classification via convolutional neural networks with simple negative sampling.&quot; arXiv preprint arXiv:1506.07650 (2015).][]</li><li>‚≠ê[Lin, Yankai, et al. &quot;Neural relation extraction with selective attention over instances.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.][]</li></ol><h5 id="Recurrent-Neural-Network"><a class="docs-heading-anchor" href="#Recurrent-Neural-Network">Recurrent Neural Network</a><a id="Recurrent-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent-Neural-Network" title="Permalink"></a></h5><ol><li>[Zhang, Dongxu, and Dong Wang. &quot;Relation classification via recurrent neural network.&quot; arXiv preprint arXiv:1508.01006 (2015).][]</li><li>[Zhang, Shu, et al. &quot;Bidirectional long short-term memory networks for relation classification.&quot; Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation. 2015.][]</li><li>[Xu, Yan, et al. &quot;Classifying relations via long short term memory networks along shortest dependency paths.&quot; Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.][]</li><li>[Zhou, Peng, et al. &quot;Attention-based bidirectional long short-term memory networks for relation classification.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Vol. 2. 2016.][]</li></ol><h5 id="Joint-Extraction/End-to-End"><a class="docs-heading-anchor" href="#Joint-Extraction/End-to-End">Joint Extraction/End-to-End</a><a id="Joint-Extraction/End-to-End-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-Extraction/End-to-End" title="Permalink"></a></h5><ol><li>[Roth, Dan, and Wen-tau Yih. A linear programming formulation for global inference in natural language tasks. ILLINOIS UNIV AT URBANA-CHAMPAIGN DEPT OF COMPUTER SCIENCE, 2004.][]</li><li>[Li, Qi, and Heng Ji. &quot;Incremental joint extraction of entity mentions and relations.&quot; Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2014.][]</li><li>[Miwa, Makoto, and Yutaka Sasaki. &quot;Modeling joint entity and relation extraction with table representation.&quot; Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.][]</li><li>[Miwa, Makoto, and Mohit Bansal. &quot;End-to-end relation extraction using lstms on sequences and tree structures.&quot; arXiv preprint arXiv:1601.00770 (2016).][]</li><li>‚≠ê[Zheng, Suncong, et al. &quot;Joint entity and relation extraction based on a hybrid neural network.&quot; Neurocomputing 257 (2017): 59-66.][]</li><li>[Ma, Xuezhe, and Eduard Hovy. &quot;End-to-end sequence labeling via bi-directional lstm-cnns-crf.&quot; arXiv preprint arXiv:1603.01354 (2016).][]</li><li>[Liu, Yang, et al. &quot;Implicit Discourse Relation Classification via Multi-Task Neural Networks.&quot; AAAI. 2016.][]</li><li>‚≠ê‚≠ê[Sun, Mingming, et al. &quot;Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction.&quot; Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018.][]</li></ol><h5 id="Deep-Reinforcement-Learning"><a class="docs-heading-anchor" href="#Deep-Reinforcement-Learning">Deep Reinforcement Learning</a><a id="Deep-Reinforcement-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-Reinforcement-Learning" title="Permalink"></a></h5><ol><li>‚≠ê‚≠ê‚≠ê[Zeng, Xiangrong, et al. &quot;Large Scaled Relation Extraction with Reinforcement Learning.&quot; Relation 2 (2018): 3.][]</li><li>‚≠ê‚≠ê[Feng, Jun, et al. &quot;Reinforcement Learning for Relation Classification from Noisy Data.&quot; (2018).][]</li><li>‚≠ê‚≠ê‚≠ê[Narasimhan, Karthik, Adam Yala, and Regina Barzilay. &quot;Improving information extraction by acquiring external evidence with reinforcement learning.&quot; arXiv preprint arXiv:1603.07954 (2016).][]</li><li>[Feng, Yuntian, et al. &quot;Joint Extraction of Entities and Relations Using Reinforcement Learning and Deep Learning.&quot; Computational intelligence and neuroscience 2017 (2017).][]</li></ol><h3 id="Unsupervised-Approaches"><a class="docs-heading-anchor" href="#Unsupervised-Approaches">Unsupervised Approaches</a><a id="Unsupervised-Approaches-1"></a><a class="docs-heading-anchor-permalink" href="#Unsupervised-Approaches" title="Permalink"></a></h3><p><strong>Pros</strong>: Leverage large amounts of data and extract large numbers of relations.</p><ol><li>[Yates, Alexander, et al. &quot;Textrunner: open information extraction on the web.&quot; Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. Association for Computational Linguistics, 2007.][]</li></ol><p>Learn a <strong>CRF</strong> over POS tags and NP chunks. A Self-Supervised Classifier is also used.</p><ol><li>[Yan, Yulan, et al. &quot;Unsupervised relation extraction by mining wikipedia texts using information from the web.&quot; Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009.][]</li><li>[Kinoshita, Keisuke, et al. &quot;The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech.&quot; Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013 IEEE Workshop on. IEEE, 2013.][]</li></ol><p>Address some issues in TextRunner.(Read the paper for details.)</p><ol><li>[Angeli, Gabor, Melvin Jose Johnson Premkumar, and Christopher D. Manning. &quot;Leveraging linguistic structure for open domain information extraction.&quot; Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vol. 1. 2015.][]</li></ol><p>Attention is first introduced.</p><ol><li>[Wang, Linlin, et al. &quot;Relation classification via multi-level attention cnns.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.][]</li></ol><h3 id="Distant-Supervision-Approaches"><a class="docs-heading-anchor" href="#Distant-Supervision-Approaches">Distant Supervision Approaches</a><a id="Distant-Supervision-Approaches-1"></a><a class="docs-heading-anchor-permalink" href="#Distant-Supervision-Approaches" title="Permalink"></a></h3><ol><li>[Craven, Mark, and Johan Kumlien. &quot;Constructing biological knowledge bases by extracting information from text sources.&quot; ISMB. Vol. 1999. 1999.][]</li><li>‚≠ê‚≠ê‚≠ê[Mintz, Mike, et al. &quot;Distant supervision for relation extraction without labeled data.&quot; Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009.][]<blockquote><p>If two entities participate in a relation, <strong>any sentence</strong> that contains those two entities might express that relation.</p></blockquote>In fact, it is supervised by a database, rather than by labeled text.  <img src="Resources/DistantSupervisionOverview.png" alt="DistantSupervisionOverview.png"/></li><li>‚≠ê[Riedel, Sebastian, Limin Yao, and Andrew McCallum. &quot;Modeling relations and their mentions without labeled text.&quot; Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2010.][]<blockquote><p>If two entities participate in a relation, <strong>at least one sentence</strong> that mentions these two entities might express that relation.</p></blockquote></li><li>‚≠ê[Hoffmann, Raphael, et al. &quot;Knowledge-based weak supervision for information extraction of overlapping relations.&quot; Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011.][]</li><li>‚≠ê[Surdeanu, Mihai, et al. &quot;Multi-instance multi-label learning for relation extraction.&quot; Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.][]</li><li>[Le Sun, Xianpei Han. &quot;Global Distant Supervision for Relation Extraction.&quot; (2016).][]</li><li>[Zhang, Hongjun, et al. &quot;Relation extraction with deep reinforcement learning.&quot; IEICE TRANSACTIONS on Information and Systems 100.8 (2017): 1893-1902.][]</li><li>[Ji, Guoliang, et al. &quot;Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions.&quot; AAAI. 2017.][]</li></ol><h3 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h3><p><img src="Resources/comparision.png" alt="comparision.png"/></p><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><ul><li><a href="Data/SemEval2010_task8_all_data.zip">SemEval2010_task8</a></li><li><a href="http://lipn.univ-paris13.fr/~gabor/semeval2018task7/">SemEval2018_task7</a></li><li><a href="https://catalog.ldc.upenn.edu/ldc2005t09">ACE 2004</a> (<strong>NOT FREE</strong>)</li><li><a href="https://catalog.ldc.upenn.edu/LDC2006T06">ACE 2005</a> (<strong>NOT FREE</strong>)</li></ul><h2 id="Tools"><a class="docs-heading-anchor" href="#Tools">Tools</a><a id="Tools-1"></a><a class="docs-heading-anchor-permalink" href="#Tools" title="Permalink"></a></h2><h3 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h3><ol><li>‚≠ê‚≠ê‚≠ê<a href="https://github.com/dair-iitd/OpenIE-standalone">OpenIE5.0</a></li><li>‚≠ê<a href="http://nlp.stanford.edu/software/relationExtractor.html">Stanford relation extractor</a></li><li><a href="https://nlp.stanford.edu/software/openie.html">Stanford Open Information Extraction</a></li><li><a href="https://github.com/knowitall/ollie">Ollie</a></li><li><a href="http://reverb.cs.washington.edu/">ReVerb</a></li><li><a href="http://deepdive.stanford.edu/">DeepDive</a></li></ol><h3 id="Code"><a class="docs-heading-anchor" href="#Code">Code</a><a id="Code-1"></a><a class="docs-heading-anchor-permalink" href="#Code" title="Permalink"></a></h3><ol><li>‚≠ê‚≠ê<a href="https://github.com/thunlp/TensorFlow-NRE">Neural Relation Extraction</a></li><li><a href="https://github.com/JuneFeng/RelationClassification-RL">Reinforcement Learning for Relation Classification from Noisy Data(AAAI2018)</a></li><li><a href="https://github.com/karthikncode/DeepRL-InformationExtraction">DeepRL-InformationExtraction</a></li></ol><p>[Maynard, Diana, Kalina Bontcheva, and Isabelle Augenstein. &quot;Natural language processing for the semantic web.&quot; Synthesis Lectures on the Semantic Web: Theory and Technology 6.2 (2016): 1-194.]: Paper/NaturalLanguageProcessingfortheSemanticWeb.pdf [Brin, Sergey. &quot;Extracting patterns and relations from the world wide web.&quot; International Workshop on The World Wide Web and Databases. Springer, Berlin, Heidelberg, 1998.]: Paper/1999-65.pdf [Etzioni, Oren, et al. &quot;Web-scale information extraction in knowitall:(preliminary results).&quot; Proceedings of the 13th international conference on World Wide Web. ACM, 2004.]: Paper/1p100.pdf [Carlson, Andrew, et al. &quot;Toward an architecture for never-ending language learning.&quot; AAAI. Vol. 5. 2010.]: Paper/1879-8287-1-PB.pdf [Pedro, Saulo DS, and Estevam R. Hruschka. &quot;Conversing learning: Active learning and active social interaction for human supervision in never-ending learning systems.&quot; Ibero-American Conference on Artificial Intelligence. Springer, Berlin, Heidelberg, 2012.]: Paper/iberamia2012.pdf [Reiss, Frederick, et al. &quot;An algebraic approach to rule-based information extraction.&quot; Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on. IEEE, 2008.]: Paper/48e62ae4478bfad44adfec18b156f471a68c.pdf [Dong, Xin, et al. &quot;Knowledge vault: A web-scale approach to probabilistic knowledge fusion.&quot; Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.]: Paper/Dong2014KVW.pdf [Yates, Alexander, et al. &quot;Textrunner: open information extraction on the web.&quot; Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. Association for Computational Linguistics, 2007.]: Paper/p25-yates.pdf [Kinoshita, Keisuke, et al. &quot;The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech.&quot; Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013 IEEE Workshop on. IEEE, 2013.]: Paper/8eff98cfd960ba7ab004b213ef1d1a76f17b.pdf [Angeli, Gabor, Melvin Jose Johnson Premkumar, and Christopher D. Manning. &quot;Leveraging linguistic structure for open domain information extraction.&quot; Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vol. 1. 2015.]: Paper/P15-1034.pdf [Craven, Mark, and Johan Kumlien. &quot;Constructing biological knowledge bases by extracting information from text sources.&quot; ISMB. Vol. 1999. 1999.]: Paper/ISMB99-010.pdf [Mintz, Mike, et al. &quot;Distant supervision for relation extraction without labeled data.&quot; Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009.]: Paper/P09-1113.pdf [Li, Juanzi, et al. &quot;Knowledge Graph and Semantic Computing.&quot;]: Paper/knowledgegraphandsemanticcomputing.pdf [Yan, Yulan, et al. &quot;Unsupervised relation extraction by mining wikipedia texts using information from the web.&quot; Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009.]: Paper/P09-1115.pdf [Kambhatla, Nanda. &quot;Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.&quot; Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics, 2004.]: Paper/P04-3022.pdf [Riedel, Sebastian, Limin Yao, and Andrew McCallum. &quot;Modeling relations and their mentions without labeled text.&quot; Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2010.]: Paper/b9a731678bf0494fe29cbebb42a822224cc6.pdf [Hoffmann, Raphael, et al. &quot;Knowledge-based weak supervision for information extraction of overlapping relations.&quot; Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011.]: Paper/P11-1055.pdf [Surdeanu, Mihai, et al. &quot;Multi-instance multi-label learning for relation extraction.&quot; Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.]: Paper/D12-1042.pdf [Le Sun, Xianpei Han. &quot;Global Distant Supervision for Relation Extraction.&quot; (2016).]: Paper/12006-56239-1-PB.pdf [Socher, Richard, et al. &quot;Semantic compositionality through recursive matrix-vector spaces.&quot; Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.]: Paper/D12-1110.pdf [Zeng, Daojian, et al. &quot;Relation classification via convolutional deep neural network.&quot; Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. 2014.]: Paper/C14-1220.pdf [Nguyen, Thien Huu, and Ralph Grishman. &quot;Relation extraction: Perspective from convolutional neural networks.&quot; Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing. 2015.]: Paper/W15-1506.pdf [Santos, Cicero Nogueira dos, Bing Xiang, and Bowen Zhou. &quot;Classifying relations by ranking with convolutional neural networks.&quot; arXiv preprint arXiv:1504.06580 (2015).]: Paper/1504.06580.pdf [Xu, Kun, et al. &quot;Semantic relation classification via convolutional neural networks with simple negative sampling.&quot; arXiv preprint arXiv:1506.07650 (2015).]: Paper/1506.07650.pdf [Lin, Yankai, et al. &quot;Neural relation extraction with selective attention over instances.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.]: Paper/P16-1200.pdf [Wang, Linlin, et al. &quot;Relation classification via multi-level attention cnns.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.]: Paper/P16-1123.pdf [Zhang, Dongxu, and Dong Wang. &quot;Relation classification via recurrent neural network.&quot; arXiv preprint arXiv:1508.01006 (2015).]: Paper/1508.01006.pdf [Zhang, Shu, et al. &quot;Bidirectional long short-term memory networks for relation classification.&quot; Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation. 2015.]: Paper/Y15-1009.pdf [Xu, Yan, et al. &quot;Classifying relations via long short term memory networks along shortest dependency paths.&quot; Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.]: Paper/D15-1206.pdf [Zhou, Peng, et al. &quot;Attention-based bidirectional long short-term memory networks for relation classification.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Vol. 2. 2016.]: Paper/P16-2034.pdf [Li, Qi, and Heng Ji. &quot;Incremental joint extraction of entity mentions and relations.&quot; Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2014.]: Paper/P14-1038.pdf [Miwa, Makoto, and Yutaka Sasaki. &quot;Modeling joint entity and relation extraction with table representation.&quot; Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.]: Paper/D14-1200.pdf [Miwa, Makoto, and Mohit Bansal. &quot;End-to-end relation extraction using lstms on sequences and tree structures.&quot; arXiv preprint arXiv:1601.00770 (2016).]: Paper/1601.00770.pdf [Zheng, Suncong, et al. &quot;Joint entity and relation extraction based on a hybrid neural network.&quot; Neurocomputing 257 (2017): 59-66.]: Paper/Joint-Entity-and-Relation-Extraction-Based-on.pdf [Ma, Xuezhe, and Eduard Hovy. &quot;End-to-end sequence labeling via bi-directional lstm-cnns-crf.&quot; arXiv preprint arXiv:1603.01354 (2016).]: Paper/1603.01354.pdf [Kumar, Shantanu. &quot;A Survey of Deep Learning Methods for Relation Extraction.&quot; arXiv preprint arXiv:1705.03645 (2017).]: Paper/1705.03645.pdf [Liu, Yang, et al. &quot;Implicit Discourse Relation Classification via Multi-Task Neural Networks.&quot; AAAI. 2016.]: Paper/11831-56211-1-PB.pdf [Zhang, Dongxu, and Dong Wang. &quot;Relation Classification: CNN or RNN?.&quot; Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016. 665-675.]: Paper/213.pdf [Feng, Jun, et al. &quot;Reinforcement Learning for Relation Classification from Noisy Data.&quot; (2018).]: Paper/AAAI2018Denoising.pdf [Pawar, Sachin, Girish K. Palshikar, and Pushpak Bhattacharyya. &quot;Relation Extraction: A Survey.&quot; arXiv preprint arXiv:1712.05191 (2017).]: Paper/1712.05191.pdf [Zeng, Xiangrong, et al. &quot;Large Scaled Relation Extraction with Reinforcement Learning.&quot; Relation 2 (2018): 3.]: Paper/zeng<em>aaai2018.pdf [Narasimhan, Karthik, Adam Yala, and Regina Barzilay. &quot;Improving information extraction by acquiring external evidence with reinforcement learning.&quot; arXiv preprint arXiv:1603.07954 (2016).]: Paper/1603.07954.pdf [Zhang, Hongjun, et al. &quot;Relation extraction with deep reinforcement learning.&quot; IEICE TRANSACTIONS on Information and Systems 100.8 (2017): 1893-1902.]: Paper/E100.D</em>2016EDP7450.pdf [Feng, Yuntian, et al. &quot;Joint Extraction of Entities and Relations Using Reinforcement Learning and Deep Learning.&quot; Computational intelligence and neuroscience 2017 (2017).]: Paper/7643065.pdf [Sun, Mingming, et al. &quot;Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction.&quot; Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018.]: Paper/2018<em>Logician</em>A<em>Unified</em>End-to-End<em>Neural</em>Approach<em>for</em>open<em>domain</em>IE(1).pdf [Ji, Guoliang, et al. &quot;Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions.&quot; AAAI. 2017. ]: Paper/14491-66562-1-PB.pdf [Hashimoto, Kazuma, et al. &quot;Simple customization of recursive neural networks for semantic relation classification.&quot; Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. 2013.]: Paper/D13-1137.pdf [Roth, Dan, and Wen-tau Yih. A linear programming formulation for global inference in natural language tasks. ILLINOIS UNIV AT URBANA-CHAMPAIGN DEPT OF COMPUTER SCIENCE, 2004.]: Paper/ADA460702.pdf [Soderland, Stephen G. Learning text analysis rules for domain-specific natural language processing. Diss. University of Massachusetts at Amherst, 1997.]: Paper/10.1.1.48.8283.pdf</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
