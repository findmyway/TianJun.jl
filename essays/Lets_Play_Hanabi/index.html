<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Let&#39;s Play Hanabi! ¬∑ Jun Tian</title><meta name="title" content="Let&#39;s Play Hanabi! ¬∑ Jun Tian"/><meta property="og:title" content="Let&#39;s Play Hanabi! ¬∑ Jun Tian"/><meta property="twitter:title" content="Let&#39;s Play Hanabi! ¬∑ Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">üëã About</a></li><li><span class="tocitem">üíª Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">üìñ Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">üôã Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">üîó Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Let&#39;s Play Hanabi!</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Let&#39;s Play Hanabi!</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/Lets_Play_Hanabi/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: Game,ReinforcementLearning,Hanabi CJKmainfont: KaiTi ‚Äì-</p><h1 id="Let&#39;s-Play-Hanabi!"><a class="docs-heading-anchor" href="#Let&#39;s-Play-Hanabi!">Let&#39;s Play Hanabi!</a><a id="Let&#39;s-Play-Hanabi!-1"></a><a class="docs-heading-anchor-permalink" href="#Let&#39;s-Play-Hanabi!" title="Permalink"></a></h1><p>Papers are removed. (2021-08-12)</p><p>This blog provides some detailed information for my lighting talk on JuliaCon 2019 (<a href="https://pretalx.com/juliacon2019/talk/8T3FVZ/">Let&#39;s Play Hanabi!</a>).</p><p>You may find the slide <a href="slide/LetsPlayHanabi.pdf">here</a> and the source code <a href="https://github.com/findmyway/LetsPlayHanabi">here</a>.</p><h2 id="The-Rainbow-Algorithm"><a class="docs-heading-anchor" href="#The-Rainbow-Algorithm">The Rainbow Algorithm</a><a id="The-Rainbow-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#The-Rainbow-Algorithm" title="Permalink"></a></h2><p>Writing the Rainbow algorithm in Julia is relatively easy. It&#39;s just three dense layer together with a projection step. Although the structure is simple, it&#39;s not always that easy to make it work as we expect. Many parameters need to be tuned and some steps are not well documented in the original paper. (The evil lives in details!) You may read <a href="https://danieltakeshi.github.io/2019/07/14/per/">Understanding Prioritized Experience Replay</a> to get a better understanding of what I mean here. To make things easier, I just keep all the parameters and loss calculation step the same with the original implementation in <a href="https://github.com/deepmind/hanabi-learning-environment">deepmind/hanabi-learning-environment</a>. And the result shows that these two implementation behaves similarly, except the speed.</p><p>With CPU only, the Flux based implementation is much slower compared to the TensorFlow based one. One of the most important reason is that TensorFlow will utilize available CPU as much as possible, even I manually change <code>tf.device(&quot;/cpu:*&quot;)</code> into <code>tf.device(&quot;/cpu:0&quot;)</code> or <code>tf.device(&quot;/cpu:1&quot;)</code>. So instead, I rerun the TensorFlow code in a docker environment with only 1 cpu allocated (by setting <code>--cpuset-cpus=0</code>) and the training speed decreased from 258 steps per second into 200. There&#39;s still an obvious advantage compared to my Julia implementation. I did some benchmark, it seems that the backpropagation step was the bottleneck. I tried to switch the <code>Tracker</code> into <code>Zygote</code>, hoping that it could be faster. However a strange error occurred and I was not sure how to fix it at that time.</p><p>With GPU enabled, the Flux based implementation is much faster. The speedup comes from two parts: the fused broadcast and <code>@view</code>. With fused broadcast, we can speed up the projection step. In theory, if we turn on the XLA in TensorFlow, we should witness similar improvement in the TensorFlow version. But I found that simply using the session config as described <a href="https://www.tensorflow.org/xla/jit#session">here</a> in the original implementation seems not work. I guess that the reason is that some CPU computing steps are included in the training op. I haven&#39;t tried to manually optimize only the projection step yet.</p><p>The more interesting thing is that, the same Julia code runs faster on RTX 2080 TI compared to V100. The benchmark of a simple matrix multiplication shows that, for the size of 512 * 512 (the hidden layer size in my problem) it is about 1/3 faster on RTX 2080 TI. But for some large matrix multiplication, say 8192 * 8192, there isn&#39;t much difference on these two cards.</p><h2 id="Distributed-Experience-Replay-Buffer"><a class="docs-heading-anchor" href="#Distributed-Experience-Replay-Buffer">Distributed Experience Replay Buffer</a><a id="Distributed-Experience-Replay-Buffer-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Experience-Replay-Buffer" title="Permalink"></a></h2><p>You may think it is relatively easy to adapt the Rainbow algorithm to the distributed version, after all Julia has a good support for parallel computing. But after some attempts, I must admit that there&#39;s no easy way. If we choose the multi-threading based implementation, obviously it can&#39;t be applied to multiple machines. If we choose the multi-processing, then the performance is not that good (I&#39;ll explain it soon). So it seems that a hybrid mode would be great, but that needs a delicate design.</p><p>Th Ape-X contains a learner and multiple actors. Each one can live in an independent processor. The difficult part is how to communicate between learner and actors. Here learner and actor both run very fast. The learner runs on a GPU and the actor only do forward inference. And the communication contains four parts:</p><h3 id="Parameter-Sharing"><a class="docs-heading-anchor" href="#Parameter-Sharing">Parameter Sharing</a><a id="Parameter-Sharing-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Sharing" title="Permalink"></a></h3><p>Actors need to update their parameters periodically from the learner. Assuming that we have hundreds of actors, each time an actor invoke a remote call, it will slow down the speed of the learner. So the best way is that the learner periodically send it&#39;s parameters to a intermediate scheduler and let it communicate with other actors.</p><h3 id="Experience-Updating"><a class="docs-heading-anchor" href="#Experience-Updating">Experience Updating</a><a id="Experience-Updating-1"></a><a class="docs-heading-anchor-permalink" href="#Experience-Updating" title="Permalink"></a></h3><p>Actors will generate experiences as fast as possible with priorities pre-calculated. Then those experiences are cached locally for a while and sent to the global Prioritized Experience Replay Buffer. There shouldn&#39;t be any problem if the global buffer is an independent processor.</p><h3 id="Sample-Generation"><a class="docs-heading-anchor" href="#Sample-Generation">Sample Generation</a><a id="Sample-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-Generation" title="Permalink"></a></h3><p>If the global Prioritized Experience Replay Buffer is in the same processor, then the experience updating step will slow down the learner. Otherwise, there&#39;s a overhead to copy experiences to the learner. I found it hard to balance these two.</p><h3 id="Experience-Priority-Updating"><a class="docs-heading-anchor" href="#Experience-Priority-Updating">Experience Priority Updating</a><a id="Experience-Priority-Updating-1"></a><a class="docs-heading-anchor-permalink" href="#Experience-Priority-Updating" title="Permalink"></a></h3><p>All the experiences consumed by the learner will be updated with new priorities.</p><p>As you can see, there are multiple steps updating the global experience replay buffer simultaneously. In the <a href="https://github.com/belepi93/Ape-X">pytorch</a> implementation, there&#39;s a good picture demonstrating each component.</p><p>My feeling is that, it&#39;s really hard to keep the speed of learner and actor well balanced.</p><h2 id="Bayesian-Action-Decoder"><a class="docs-heading-anchor" href="#Bayesian-Action-Decoder">Bayesian Action Decoder</a><a id="Bayesian-Action-Decoder-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Action-Decoder" title="Permalink"></a></h2><p>This method is really elegant. The only problem is that it runs too slow. I&#39;ll update this part once I finished changing the original policy gradient based implementation into value based methods.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
