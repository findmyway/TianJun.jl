<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Paraphrase Generation ¬∑ Jun Tian</title><meta name="title" content="Paraphrase Generation ¬∑ Jun Tian"/><meta property="og:title" content="Paraphrase Generation ¬∑ Jun Tian"/><meta property="twitter:title" content="Paraphrase Generation ¬∑ Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">üëã About</a></li><li><span class="tocitem">üíª Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">üìñ Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">üôã Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">üîó Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Paraphrase Generation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Paraphrase Generation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/Paraphrase_Generation/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: NLP CJKmainfont: KaiTi ‚Äì-</p><h1 id="Paraphrase-Generation"><a class="docs-heading-anchor" href="#Paraphrase-Generation">Paraphrase Generation</a><a id="Paraphrase-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Paraphrase-Generation" title="Permalink"></a></h1><h2 id="Paper-List"><a class="docs-heading-anchor" href="#Paper-List">Paper List</a><a id="Paper-List-1"></a><a class="docs-heading-anchor-permalink" href="#Paper-List" title="Permalink"></a></h2><ul><li><p><a href="./papers/Paraphrase_Generation_with_Deep_Reinforcement_Learning.pdf">Paraphrase Generation with Deep Reinforcement Learning</a></p><ul><li><p>Neural paraphrase generation recently draws attention in different application scenarios. The task is often formalized as a sequence-to-sequence (Seq2Seq) learning problem. Prakash et al. (2016) employ a stacked residual LSTM network in the Seq2Seq model to enlarge the model capacity. Cao et al. (2017) utilize an additional vocabulary to restrict word candidates during generation. Gupta et al. (2018) use a variational auto-encoder framework to generate more diverse paraphrases. Ma et al. (2018) utilize an attention layer instead of a linear mapping in the decoder to pick up word candidates. Iyyer et al. (2018) harness syntactic information for controllable paraphrase generation. Zhang and Lapata (2017) tackle a similar task of sentence simplification withe Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning.</p></li><li><p>There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pivoting over a shared translation in another language. Wieting et al. (2017); Wieting and Gimpel (2018) use neural machine translation to generate paraphrases via back-translation of bilingual sentence pairs. Buck et al. (2018) and Dong et al. (2017) tackle the problem of QA-specific paraphrasing with the guidance from an external QA system and an associated evaluation metric.</p></li></ul></li><li><p><a href="./papers/Semantic_Parsing_via_Paraphrasing.pdf">Semantic Parsing via Paraphrasing</a></p><ul><li>Canonical utterance construction Given an utterance x and the KB, we construct a set of candidate logical forms Zx, and then for each z 2 Zx generate a small set of canonical natural language utterances Cz. Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utterance from it. This strategy is feasible in factoid QA where compositionality is low, and so the size of Zx is limited (Section 4)</li><li>Paraphrasing We score the canonical utterances in Cz with respect to the input utterance x using a paraphrase model, which offers two advantages. First, the paraphrase model is decoupled from the KB, so we can train it from large text corpora. Second, natural language utterances often do not express predicates explicitly, e.g., the question ‚ÄúWhat is Italy‚Äôs money?‚Äù expresses the binary predicate CurrencyOf with a possessive construction. Paraphrasing methods are well-suited for handling such text-to-text gaps. </li></ul></li><li><p><a href="./slides/Paraphrase_Detection_in_NLP.pdf">Paraphrase Detection in NLP(Slide)</a></p></li><li><p><a href="./papers/Neural_Paraphrase_Generation_with_Stacked_Residual_LSTM_Networks.pdf">Neural Paraphrase Generation with Stacked Residual LSTM Networks(2016)</a></p><ul><li>Some <em>old</em> encoder decoder methods</li></ul></li><li><p><a href="./papers/Learning_Semantic_Sentence_Embeddings_using_Pair_wise_Discriminator.pdf">Learning Semantic Sentence Embeddings using Pair-wise Discriminator</a></p><ul><li>An application of paraphrase</li></ul><p><img src="img/Pair-wise.png" alt/></p></li><li><p><a href="./papers/Learning_Paraphrastic_Sentence_Embeddings_from_Back_Translated_Bitext.pdf">Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</a>   <img src="img/back_translation.png" alt/></p><ul><li>...finding clear differences in length, the amount of repetition, and the use of rare words.</li></ul></li><li><p><a href="./papers/Pushing_the_Limits_of_Paraphrastic_Sentence_Embeddings_with_Millions_of_Machine_Translations.pdf">Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations</a>   Much larger compared above</p></li><li><p><a href="./papers/Deep_Reinforcement_Learning_for_Sequence_to_Sequence_Models.pdf">Deep Reinforcement Learning for Sequence to Sequence Models</a></p><ul><li>Using RL to address the following two questions:<ul><li><ol><li>exposure bias</li></ol></li><li><ol><li>inconsistency between train/test measurement</li></ol></li></ul></li></ul></li><li><p><a href="./papers/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a></p><ul><li>First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator.</li><li>Second, we use coverage to keep track of what has been summarized, which discourages repetition.</li></ul></li><li><p><a href="./papers/A_Deep_Generative_Framework_for_Paraphrase_Generation.pdf">A Deep Generative Framework for Paraphrase Generation</a>   <img src="img/VAE_LSTM.png" alt/></p></li><li><p><a href="./papers/Semantic_Structural_Evaluation_for_Text_Simplification.pdf">Semantic Structural Evaluation for Text Simplification</a>   We presented the first structure-aware metric for</p></li></ul><p>text simplification, SAMSA, and the first evaluation experiments that directly target the structural simplification component, separately from the lexical component. </p><ul><li><a href="./papers/Dynamic_Multi_Level_Multi_Task_Learning_for_Sentence_Simplification.pdf">Dynamic Multi-Level Multi-Task Learning for Sentence Simplification</a>   <img src="img/MultiTaskSimplification.png" alt/></li><li><a href="./papers/Integrating_Transformer_and_Paraphrase_Rules_for_sentence_simplification.pdf">Integrating Transformer and Paraphrase Rules for Sentence Simplification</a></li><li><a href="./papers/Conditional_Text_Paraphrasing_A_Survey_and_Taxonomy.pdf">Conditional Text Paraphrasing A Survey and Taxonomy</a>   <img src="img/ConditionalTextParaphrasing.png" alt/></li><li><a href="./papers/A_Deep_Ensemble_Model_with_Slot_Alignment_for_Sequence-to-Sequence_Natural_Language_Generation.pdf">A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation</a><ul><li>Slot Alignment</li></ul></li><li><a href="./papers/Evaluating_the_State-of-the-Art_of_End-to-End_Natural_Language_Generation_The_E2E_NLG_Challenge.pdf">Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge(80 pages /2019/07/24)</a></li><li><a href="./papers/Constrained-Decoding-for-Neural-NLG-from-Compositional-Representations-in-Task-Oriented-Dialogue.pdf">Constrained-Decoding-for-Neural-NLG-from-Compositional-Representations-in-Task-Oriented-Dialogue.pdf</a><ul><li>(1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning;</li><li>(2) introduce a challenging dataset using this representation for the weather domain;</li><li>(3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness;</li><li>(4) demonstrate promising results on our dataset and the E2E dataset</li></ul></li><li><a href="./papers/Syntactic_Manipulation_for_Generating_More_Diverse_and_Interesting_Texts.pdf">Syntactic Manipulation for Generating More Diverse and Interesting Texts</a><ul><li>Syntactic Controled LSTM</li></ul><img src="img/SyntacticControledLSTM.png" alt/></li></ul></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
