<!DOCTYPE html>
<html lang="en-US"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Computer Age Statistical Inference 读书笔记 · Jun Tian</title><meta name="title" content="Computer Age Statistical Inference 读书笔记 · Jun Tian"/><meta property="og:title" content="Computer Age Statistical Inference 读书笔记 · Jun Tian"/><meta property="twitter:title" content="Computer Age Statistical Inference 读书笔记 · Jun Tian"/><meta name="description" content="Documentation for Jun Tian."/><meta property="og:description" content="Documentation for Jun Tian."/><meta property="twitter:description" content="Documentation for Jun Tian."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132847825-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-132847825-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpg" alt="Jun Tian logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Jun Tian</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">👋 About</a></li><li><span class="tocitem">💻 Programming</span><ul><li><a class="tocitem" href="../../programming/A_Deep_Dive_into_Distributed.jl/">A Deep Dive into Distributed.jl</a></li></ul></li><li><span class="tocitem">📖 Reading</span><ul><li><a class="tocitem" href="../../reading/Notes_on_Distributional_Reinforcement_Learning/">Notes on Distributional Reinforcement Learning</a></li></ul></li><li><a class="tocitem" href="../../AMA/">🙋 Ask Me Anything</a></li><li><a class="tocitem" href="../../blogroll/">🔗 Blogroll</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href><em>Computer Age Statistical Inference</em> 读书笔记</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href><em>Computer Age Statistical Inference</em> 读书笔记</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/findmyway/TianJun.jl/blob/master/docs/src/essays/Notes_on_Computer_Age_Statistical_Inference/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><hr/><p>keywords: Book CJKmainfont: KaiTi –-</p><h1 id="*Computer-Age-Statistical-Inference*-读书笔记"><a class="docs-heading-anchor" href="#*Computer-Age-Statistical-Inference*-读书笔记"><em>Computer Age Statistical Inference</em> 读书笔记</a><a id="*Computer-Age-Statistical-Inference*-读书笔记-1"></a><a class="docs-heading-anchor-permalink" href="#*Computer-Age-Statistical-Inference*-读书笔记" title="Permalink"></a></h1><p>从MS Library借的这本书，两个作者都很给力，Trevor是ESL的合作者之一，Bradley是bootstrap的inventor。这本书是按照时间顺序展开的，读完有利于对过去几十年里统计推断的一些发展有个更清晰的脉络。这里随手记点笔记。</p><p>这本书有对应的<a href="https://web.stanford.edu/~hastie/CASI/index.html">网站</a>，可以在上面查看电子版，数据集以及讨论等（评论用的disqus，所以需要翻墙的）。</p><p>前言里有句话挺有意思：</p><blockquote><p>Very broadly speaking, algorithms are what statisticians do while inference says why they do them.</p></blockquote><h1 id="Ch1-Algorithms-and-Inference"><a class="docs-heading-anchor" href="#Ch1-Algorithms-and-Inference">Ch1 Algorithms and Inference</a><a id="Ch1-Algorithms-and-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Ch1-Algorithms-and-Inference" title="Permalink"></a></h1><p>书中举了个回归的例子，先用线性回归拟合了年龄与肾脏Tot指数之间的关系。顺便复习下标准差的计算：</p><p>$</p><p>\begin{equation} \sigma<em>{\bar{x}} = \frac{\sigma}{\sqrt{n}}                \approx \sqrt{\frac {\sum</em>{i=1}^{n} (x_i - \bar{x})^2} {n(n-1)} } \end{equation} \label{se} $</p><p>上式中，<span>$\sigma$</span>是整体的标准差，这里用样本的标准差来近似。关于如何计算线性估计的置信区间，可以看看<a href="https://en.wikipedia.org/wiki/Simple_linear_regression">Simple<em>linear</em>regression</a>，以及<a href="Standard Errors for Regression Equations.pdf">Standard Errors for Regression Equations.pdf</a>。下面我用Julialang复现了下图一。</p><pre><code class="language-julia hljs">using CSV
using Plots
using StatPlots
using DataFrames
using GLM

gr()
cd(raw&quot;D:\workspace\github\blog-py\blog\static\essay_resources\Notes_on_Computer_Age_Statistical_Inference&quot;) 

kidney = CSV.read(&quot;kidney.csv&quot;, nullable=false)
@df kidney scatter(:age, :Tot)

X = hcat(ones(nrow(kidney)), kidney[:age])
y = kidney[:Tot]
OLS = fit(LinearModel, X, y)
# GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}}:

# Coefficients:
#        Estimate Std.Error  t value Pr(&gt;|t|)
# x1      2.86067  0.359561  7.95603   &lt;1e-12
# x2   -0.0786009 0.0090557 -8.67972   &lt;1e-14

age_samples = collect(20:10:90)
Xtest = hcat(ones(length(age_samples)), age_samples)
pred = predict(OLS, Xtest, :confint)

for i in 1:size(pred, 1)
    y_pred, y_lower, y_upper = pred[i, :]
    display(plot!([age_samples[i],age_samples[i]], [ y_lower, y_upper], linewidth = 3))
end

plot!(age_samples[[1, end]], pred[[1, end], 1], legend=:none, linewidth=3)
savefig(&quot;Figure_1_1.png&quot;)</code></pre><p><img src="Figure_1_1.png" alt="Figure_1_1"/></p><p>此外书中还用lowess和bootstrap方法拟合了该数据集，暂时对这二者不太熟，后面深入学习了再试着复现下。另外，Julia中的GLM这个库，感觉还是不够完善，对DataFrame的支持不是特别好，跟R语言是没法比的了，不过也还凑合，等我多一些Julia经验了去完善下。</p><p>第二个例子是假设检验。首先选取了两种白血病人中第136号基因的活跃性作为对比，根据t检验的结果，按照一般的理解，应该得出0.0036的显著性假设（即该基因的活跃性有很大的区分度）。然而，该基因只是7128个基因指标中的一个，这又让该结果显得不那么令人感到惊讶，于是捎带引出了false-discovery-rate的概念。</p><p>我的理解是，作者在第一章用这两个例子是想让读者理解<strong>Statistical Inference</strong>的概念。比如，第一个回归的例子中，用线性回归（或者其它多项式回归）来拟合观测数据（即Algorithm），然后再用Standard Error（或者lowess,bootstrap standard error等）衡量误差（即Inference）；第二个例子中，用t检验来检测Null Hypothesis，然后再用false-discovery-rate来衡量假设检验的结果。书中提到，这就有点像<a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey</a>提出的<strong>explanation-confirmation</strong>系统，当然，作者认为如今Algorithm和Inference的概念要远比这二者更广泛。</p><h1 id="Ch2-Frequentist-Inference"><a class="docs-heading-anchor" href="#Ch2-Frequentist-Inference">Ch2 Frequentist Inference</a><a id="Ch2-Frequentist-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Ch2-Frequentist-Inference" title="Permalink"></a></h1><p>假设我们观测到了211个肾脏病人的gfr（glomerular filtrate rate 肾小球过滤率）指标 <span>$\boldsymbol{x} = (x_1, x_2, ..., x_n)$</span>，该指标在所有肾脏病人中的分布为<span>$F$</span>（该分布是未知的），那么<span>$\boldsymbol{X} = (X_1, X_2,...X_n)$</span>可以看作是n次从<span>$F$</span>中独立采样的结果，记作：</p><p>$</p><p>\begin{equation} F \rightarrow \boldsymbol{X} \end{equation} $</p><p>然后，假设我们想要得到的是从<span>$F$</span>中每一次随机采样的期望（注意下面式中的<span>$X$</span>不是粗体的），即：</p><p>$</p><p>\begin{equation} \theta = E_F {X } \end{equation} $</p><p>因此，需要根据已有的观测数据<span>$\boldsymbol{x}$</span>估计出<span>$\theta$</span>，</p><p>$</p><p>\begin{equation} \hat{\theta} = t(\boldsymbol{x}) \end{equation} \label{2.4} $</p><p>接下来很关键的一点，理解原书中的公式(2.5)，即：</p><p>$</p><p>\begin{equation} \hat{\Theta} = t(\boldsymbol{X}) \end{equation} \label{2.5} $</p><p class="math-container">\[\eqref{2.4}\]</p><p>和<span>$\eqref{2.5}$</span>的联系在于<span>$\boldsymbol{x}$</span>可以看作是<span>$\boldsymbol{X}$</span>的一个sample，因而<span>$\theta$</span>也可以看作是<span>$\Theta$</span>的一个实例。这样，频率学派的Inference可以定义为:</p><blockquote><p>The accuracy of an observed estimate <span>$\hat{\theta} = t(\boldsymbol{x})$</span> is the probabilistic accuracy of <span>$\hat{\Theta} = t(\boldsymbol{X})$</span> as an estimator of <span>$\theta$</span>（这句话有点绕，好难翻译，先贴个原文）</p></blockquote><p>通常我们关心bias和variance，即：</p><p>$</p><p>\begin{equation} \mu = E_F{\hat{\Theta}} \end{equation} $</p><p>$</p><p>\begin{equation} \mathrm{bias} = \mu - \theta \quad \mathrm{and} \quad \mathrm{var} = E_F{(\hat{\Theta} - \mu )^2} \end{equation} $</p><p>实际使用中，通常会有一些折中手段，最常见的就是直接plug-in(其实就是<span>$\eqref{se}$</span>中的近似处理)</p><p>$</p><p>\begin{equation} se(\bar{X}) = [\mathrm{var}_F(X) / n]^{1/2} \end{equation} $</p><p>其中<span>$\mathrm{var}_F(X)$</span>可以根据观测样本<span>$\boldsymbol{x}$</span>来估计：</p><p>$</p><p>\begin{equation} \hat{\mathrm{var}}<em>F = \sum (x</em>i - \bar{x})^2 / (n-1) \end{equation} $</p><p>所谓的plug-in就是直接用<span>$\hat{\mathrm{var}}_F(X)$</span>去替换<span>$\mathrm{var}_F(X)$</span>。</p><p>其它的几种做法在后面章节会有提及（重点看下pivotal statistics，比较另类）。</p><p>Ch2.2部分提到的likelihood ratio的思想似乎在其它地方见到过。</p><h1 id="Ch3-Bayesian-Inference"><a class="docs-heading-anchor" href="#Ch3-Bayesian-Inference">Ch3 Bayesian Inference</a><a id="Ch3-Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Ch3-Bayesian-Inference" title="Permalink"></a></h1><p>理解贝叶斯推断和频率学推断之间关系的关键在这章：</p><p>$</p><p>\begin{equation} \mathcal{F} = { f_{\mu}(x);\ x \in \mathcal{X}, \mu \in \Omega} \end{equation} $</p><p>这里<span>$x$</span>是采样空间<span>$\mathcal{X}$</span>中的一个样本（可能是一维的，也可能是多维的），而参数<span>$\mu$</span>是参数空间<span>$\Omega$</span>中的一个采样。书中举了两个<span>$f$</span>的例子（正态分布和泊松分布）来解释<span>$\mathcal{X}$</span>和<span>$\Omega$</span>的具体含义，这里不赘述。在频率学派中，<span>$\mu$</span>是固定的，我们希望通过观测值得到其估计并推断出误差，而在贝叶斯推断中，<span>$\mu$</span>是服从某种概率分布的，其先验为<span>$g(\mu)$</span>，我们希望推断出<span>$g(\mu|x)$</span>的分布。根据贝叶斯定理可以得出：</p><p>$</p><p>\begin{equation} g(\mu|x) = g(\mu) f_{\mu}(x) / f(x), \qquad \mu \in \Omega \end{equation} $</p><p>这里<span>$f(x)$</span>是<span>$\mu$</span>在<span>$\Omega$</span>下的边缘分布。</p><p>3.2部分有意思的是第二个example，作者用心良苦，引出了均匀先验、Jeffrey先验和Triangle先验。</p><p>3.4部分对二者做了完整的比较，对于低维参数，下图非常形象（对于高维情况有所不同，书中有阐述）：</p><p><img src="Figure_3_5.png" alt/></p><h1 id="Ch4-Fisherian-Inference-and-Maximum-Likelihood-Estimation"><a class="docs-heading-anchor" href="#Ch4-Fisherian-Inference-and-Maximum-Likelihood-Estimation">Ch4 Fisherian Inference and Maximum Likelihood Estimation</a><a id="Ch4-Fisherian-Inference-and-Maximum-Likelihood-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Ch4-Fisherian-Inference-and-Maximum-Likelihood-Estimation" title="Permalink"></a></h1><p>似乎，MLE刚出来的时候并不太受待见（计算太复杂）。</p><p>Fisher Information的核心是log似然相对于x微分的variance，这部分的推导以前没接触过，只是粗略知道说，MLE估计附近近似服从<span>$\hat{\theta} \sim \mathcal{N}(\theta, \sigma^2/n)$</span>。</p><p>放在这一章介绍Fisher Inference，是因为它有点介于贝叶斯和频率派分析之间，用的是频率派的那一套，不过分析的是MLE。</p><p>这一章里还提到了Cramer–Rao lower bound，后面再回过头来详细讲讲这个。</p><h1 id="Ch5-Parametric-Models-and-Exponential-Families"><a class="docs-heading-anchor" href="#Ch5-Parametric-Models-and-Exponential-Families">Ch5 Parametric Models and Exponential Families</a><a id="Ch5-Parametric-Models-and-Exponential-Families-1"></a><a class="docs-heading-anchor-permalink" href="#Ch5-Parametric-Models-and-Exponential-Families" title="Permalink"></a></h1><p>这一章容纳的知识点有点多。目前为止所接触到模型的参数维度都还较低（不超过20维），与之对应的一个概念是<strong>非参数</strong>（nonparametric）。早期之所以青睐这类参数模型在数学上处理起来方便（mathematical tractability）。</p><p>首先介绍了一些常见的分布（Normal、Poisson、Binomial、Gamma、Beta），这里需要对这类分布之间的关系有个基本的熟悉，然后是多元正态分布的一些性质。比较重要的是式子（5.16），（5.17）和（5.18），多元正态分布可以被拆解，由此也引出了后面5.3节多参分布簇的Fisher&#39;s Information Bound，中间的推导有点复杂，不过最后的结论很重要（在其他地方有读到过），MLE<span>$\mu_1$</span>的variance总是随着冗余参数的增加而上升的，这就导致最大似然以及其它近似无偏估计的方法都会过度关注“其它”参数（就是建模过程中必要但非我们关心的参数），而如今的应用都包含上千个这类参数，因而某些情况下，有偏估计反而更合适。</p><p>此外，（5.21）的结论在后面也有用到，即<span>$\mu$</span>根据观测值<span>$x$</span>得到的后验分布也是正态分布(其均值和方差的性质在第7章有用到)：</p><p>$</p><p>\begin{equation} \mu| x \sim \mathcal{N} \left(M + \frac{A}{A + \sigma^2}(x-M), \frac{A\sigma^2}{A + \sigma^2}\right) \end{equation} $</p><p>5.4节将多项分布与单纯形（Simplex）以及泊松分布之间的联系描述得很清楚。</p><blockquote><p>Nonparametrics, and the multinomial, have played a larger role in the modern environment of large, difficult to model, data sets</p></blockquote><p>没太理解5.4节结尾这句话，留着回头再看看。最后5.5节以泊松分布为例，将前面的那些分布上升到了指数簇的一般形式，这部分深究起来，还需要补许多知识点。</p><h1 id="Ch6-Empirical-Bayes"><a class="docs-heading-anchor" href="#Ch6-Empirical-Bayes">Ch6 Empirical Bayes</a><a id="Ch6-Empirical-Bayes-1"></a><a class="docs-heading-anchor-permalink" href="#Ch6-Empirical-Bayes" title="Permalink"></a></h1><p>6.1中的例子很巧妙，<em>Robbins&#39; Formula</em>，借用泊松分布的性质，在计算边缘分布的时候将先验消去了，然后根据样本估计得出参数期望的估计。这里稍微展开讲下，6.2也会用到。</p><p>某一年中，欧洲的一家汽车保险公司有9461个投保人，其中7840人没有发生索赔，1317人有1起索赔，239人有2起索赔...(如下图所示)</p><p><img src="Table_6_1.png" alt="Counts of Claims"/></p><p>假设每个投保人在一年中索赔的次数服从以下泊松分布：</p><p>$</p><p>\begin{equation} Pr{x<em>k = x} = p</em>{\theta<em>k}(x) = e^{-\theta</em>k}\theta^x<em>k / x! \label{6</em>1} \end{equation} $</p><p>其中，<span>$\theta_k$</span>是<span>$x_k$</span>的期望（回顾下泊松分布的性质）。假设我们已经知道了<span>$\theta$</span>的先验分布<span>$g(\theta)$</span>，根据贝叶斯定理：</p><p>$</p><p>\begin{equation} E{\theta | x }= \frac{\int^\infty<em>0 \theta p</em>\theta (x) g(\theta) \ d\theta}{\int^\infty<em>0 p</em>\theta (x) g(\theta) \ d\theta} \label{6_2} \end{equation} $</p><p>然后，将<span>$\eqref{6_1}$</span>带入<span>$\eqref{6_2}$</span>中，就得到了下式：</p><p>$</p><p>\begin{equation} E{\theta | x} = (x+1) f(x+1) / f(x) \end{equation} $</p><p>其中边缘分布<span>$f(x)$</span>为：</p><p>$</p><p>\begin{equation} f(x) = \int^\infty<em>0 p</em>\theta(x) g(\theta) \ d\theta = \int^\infty_0 \left[e^{-\theta} \theta^x / x! \right] g(\theta) \ d\theta \end{equation} $</p><p>这里用<span>$x$</span>在样本中的比例来作为<span>$f(x)$</span>的估计值：</p><p>$</p><p>\begin{equation} \hat{f}(x) = y<em>x / N, \quad \mathrm{with} \ N = \sum</em>x y_x \end{equation} $</p><p>这样在不知道先验分布的情况下也完成了估计。6.2中的例子思想有点类似，但是感觉技巧性更强点......求期望的时候做了个指数展开。</p><p>6.3中用一个完整的例子阐述了如何估计先验分布的参数，作者在这里是想强调21世纪以来，统计学的一些变化（逐渐在接纳indirect evidence）。</p><h1 id="Ch7-James-Stein-Estimation-and-Ridge-Regression"><a class="docs-heading-anchor" href="#Ch7-James-Stein-Estimation-and-Ridge-Regression">Ch7 James-Stein Estimation and Ridge Regression</a><a id="Ch7-James-Stein-Estimation-and-Ridge-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Ch7-James-Stein-Estimation-and-Ridge-Regression" title="Permalink"></a></h1><p>这章花了不少时间来理解，仍然有许多细节没捋清楚，先记下些要点。借助第五章的内容，先得出了<span>$\boldsymbol{\hat{\mu}} ^ {Bayes}$</span>与<span>$\boldsymbol{\hat{\mu}} ^ {MLE}$</span>的均方差期望之间相差一个系数<span>$B$</span>，其估计值为：</p><p>$</p><p>\begin{equation} \hat{B} = 1 - (N - 3) / S \qquad \left[S= \sum^N_{i=1} (x - \bar{x})^2 \right] \end{equation} $</p><p>然后<span>$\boldsymbol{\hat{\mu}}^{JS}$</span>是<span>$\boldsymbol{\hat{\mu}}^{Bayes}$</span>的一个plug-in，那么，当<span>$N \gt 3$</span>的时候，<span>$\boldsymbol{\hat{\mu}}^{JS}$</span>的risk更低（即所谓的shrinkage）。当然，根据James–Stein Theorem，该性质其实不受先验分布假设的影响。</p><p>7.2部分用一个实际的例子，阐述了James-Stein的over-shrinking特性。</p><p><img src="Figure_7_1.png" alt="Figure_7_1"/></p><p>7.3是熟悉的Ridge Regression，参数<span>$\lambda$</span>会对稀疏化程度有影响。</p><p><img src="Figure_7_2.png" alt="Figure_7_2"/></p><p>有点相当于给<span>$\beta$</span>增加在0附近的先验(当然也有许多其他解释，书中提了下就一笔带过了）。</p><p>7.4对一类Corner Case做了解释和说明，尽管risk降低了，但是毕竟是有偏估计（这在某些情况下是不能接受的）。</p><h1 id="Ch8-Generalized-Linear-Models-and-Regression-Trees"><a class="docs-heading-anchor" href="#Ch8-Generalized-Linear-Models-and-Regression-Trees">Ch8 Generalized Linear Models and Regression Trees</a><a id="Ch8-Generalized-Linear-Models-and-Regression-Trees-1"></a><a class="docs-heading-anchor-permalink" href="#Ch8-Generalized-Linear-Models-and-Regression-Trees" title="Permalink"></a></h1><p>略过，GLM相关的内容此处讲得很简略，有其它书讲得更细致。</p><h1 id="Ch9-Survival-Analysis-and-the-EM-Algorithm"><a class="docs-heading-anchor" href="#Ch9-Survival-Analysis-and-the-EM-Algorithm">Ch9 Survival Analysis and the EM Algorithm</a><a id="Ch9-Survival-Analysis-and-the-EM-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Ch9-Survival-Analysis-and-the-EM-Algorithm" title="Permalink"></a></h1><p>略过，这部分内容不是特别感兴趣。</p><h1 id="Ch10-The-Jackknife-and-the-Bootstrap"><a class="docs-heading-anchor" href="#Ch10-The-Jackknife-and-the-Bootstrap">Ch10 The Jackknife and the Bootstrap</a><a id="Ch10-The-Jackknife-and-the-Bootstrap-1"></a><a class="docs-heading-anchor-permalink" href="#Ch10-The-Jackknife-and-the-Bootstrap" title="Permalink"></a></h1><p>在有计算机之前，泰勒展开几乎是计算一些复杂指标的唯一方法。jackknife是1957年提出的，而bootstrap则是1979年。jackknife的思想很简单，但不得不佩服其开创性，从形式上有点像<em>留一交叉验证(LOOCV)</em>：</p><p>$</p><p>\begin{equation} \hat{se}<em>{jack} = \left[ \frac{n-1}{n} \sum</em>1^n \left( \hat{\theta}<em>{(i)}  - \hat{\theta}</em>{(.) }\right)^2 \right] ^{1/2} \end{equation} $</p><p>其中<span>$\hat{\theta}_{(i)}$</span>是去掉样本<span>$x_i$</span>之后的估计， <span>$\hat{\theta}_{(.)}$</span>则是前者的平均：$ \sum<em>1^n \hat{\theta}</em>{(i)} / n$</p><p>Bootstrap则往前再迈了一步，原来<span>$\hat{\theta}$</span>的估计可以看作是分两步得到的：首先从概率分布<span>$F$</span>中得到样本<span>$\boldsymbol{x}$</span>，然后根据某种计算方式<span>$s(.)$</span>得到估计值<span>$\hat{\theta}$</span>：</p><p>$</p><p>\begin{equation} F \xrightarrow{\text{iid}} \boldsymbol{x} \xrightarrow{s} \hat{\theta} \end{equation} $</p><p>而Bootstrap的做法则是，将<span>$F$</span>替换成了样本空间<span>$\hat{F}$</span>，于是计算过程为：</p><p>$</p><p>\begin{equation} \hat{F} \xrightarrow{\text{iid}} \boldsymbol{x^<em>} \xrightarrow{s} \hat{\theta}^</em> \end{equation} $</p><p>关于bootstrap总结的部分，有意思的一点是，通常，B=200足够用来估计标准差<span>$\hat{se}_{boot}$</span>，如果要计算bootstrap的置信区间，则可以需要1000或更多次的采样。</p><p>10.3中的多种重采样方案是对前面用bootstrap估计标准差的一些扩展，与前面Simplex的思想进行了统一。</p><h1 id="Ch11-Bootstrap-Confidence-Intervals"><a class="docs-heading-anchor" href="#Ch11-Bootstrap-Confidence-Intervals">Ch11 Bootstrap Confidence Intervals</a><a id="Ch11-Bootstrap-Confidence-Intervals-1"></a><a class="docs-heading-anchor-permalink" href="#Ch11-Bootstrap-Confidence-Intervals" title="Permalink"></a></h1><p>如果分布近似正态分布，那么可以用经典的<span>$\hat{\theta} \pm 1.96 \hat{se}$</span>估计出95%区间，但对于有偏分布而言，如泊松分布，该估计并不准。这一章就是解释用Bootstrap估计置信区间的一些做法。Percentile的做法比较好理解，积分后利用transformation invariance特性，即可完成估计。后面Bias-Corrected方法理解不深。11.6提到了贝叶斯区间，大概是一般教科书中都有详细阐述，这里只是简单提及了下。</p><h1 id="Ch12-Cross-Validation-and-C_p-Estimates-of-Prediction-Error"><a class="docs-heading-anchor" href="#Ch12-Cross-Validation-and-C_p-Estimates-of-Prediction-Error">Ch12 Cross-Validation and <span>$C_p$</span> Estimates of Prediction Error</a><a id="Ch12-Cross-Validation-and-C_p-Estimates-of-Prediction-Error-1"></a><a class="docs-heading-anchor-permalink" href="#Ch12-Cross-Validation-and-C_p-Estimates-of-Prediction-Error" title="Permalink"></a></h1><p>虽然一直在实验里用CV，但是很少有了解过其细节，这一章对其演变历史有了很好地阐述。12.4部分阐述的现象在以前打比赛的时候经常碰到（时序预测问题中训练集/验证集划分的问题）。</p><p><img src="Figure_12_6.png" alt="Figure_12_6"/></p><h1 id="Ch13-Objective-Bayes-Inference-and-Markov-Chain-Monte-Carlo"><a class="docs-heading-anchor" href="#Ch13-Objective-Bayes-Inference-and-Markov-Chain-Monte-Carlo">Ch13 Objective Bayes Inference and Markov Chain Monte Carlo</a><a id="Ch13-Objective-Bayes-Inference-and-Markov-Chain-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Ch13-Objective-Bayes-Inference-and-Markov-Chain-Monte-Carlo" title="Permalink"></a></h1><p>Ah, 终于到了贝叶斯推断。大多数内容在其它地方读到过，记下几点印象深刻的。</p><p>Gibbs采样的做法是将其它变量积分掉了再采样，而MCMC的做法则是先随机候选样本然后决定接受或拒绝。想要详细了解恐怕这几页是不够的，不过这本书的好处就在于提供了很丰富的参考文献。</p><p>抽象出来看，Gibbs采样和MCMC的做法相当于是从参数空间<span>$\Omega$</span>中采样得到了一个子空间<span>$A$</span>，然后替换掉贝叶斯公式中的边缘分布。（13.4有详细讲解）</p><h1 id="Ch15-Ch21"><a class="docs-heading-anchor" href="#Ch15-Ch21">Ch15 ~ Ch21</a><a id="Ch15-Ch21-1"></a><a class="docs-heading-anchor-permalink" href="#Ch15-Ch21" title="Permalink"></a></h1><p>这部分内容先不读了，几乎每一章都可以找本书来读，等以后有具体需要了再串起来读下。</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>. All contents published at this site follows <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> by default. For the Chinese version, please visit <a href="https://tianjun.me">tianjun.me</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 23 December 2023 15:51">Saturday 23 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
